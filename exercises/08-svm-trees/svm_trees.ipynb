{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "forced-retrieval",
   "metadata": {},
   "source": [
    "# Intro to scikit-learn, SVMs and decision trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "olympic-silver",
   "metadata": {},
   "source": [
    "<hr style=\"clear:both\">\n",
    "\n",
    "This notebook is part of a series of exercises for the CIVIL-226 Introduction to Machine Learning for Engineers course at EPFL. Copyright (c) 2021 [VITA](https://www.epfl.ch/labs/vita/) lab at EPFL  \n",
    "Use of this source code is governed by an MIT-style license that can be found in the LICENSE file or at https://www.opensource.org/licenses/MIT\n",
    "\n",
    "**Author(s):** [David Mizrahi](mailto:david.mizrahi@epfl.ch)\n",
    "<hr style=\"clear:both\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mighty-administrator",
   "metadata": {},
   "source": [
    "This is the final exercise of this course. In this exercise, we'll introduce the scikit-learn package, and use it to train SVMs and decision trees. We'll end with a small note on how to use scikit-learn for unsupervised learning techniques.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vietnamese-april",
   "metadata": {},
   "source": [
    "## 1. Intro to scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confident-prime",
   "metadata": {},
   "source": [
    "[scikit-learn](https://scikit-learn.org/stable/index.html) is a very popular Python package, built on top of NumPy, which provides efficient implementations of many popular machine learning algorithms.\n",
    "\n",
    "It can be used for:\n",
    "- Generating and loading popular datasets\n",
    "- Preprocessing (feature extraction and expansion, normalization)\n",
    "- Supervised learning (classification and regression)\n",
    "- Unsupervised learning (clustering and dimensionality reduction)\n",
    "- Model selection (grid search, train/test split, cross-validation)\n",
    "- Evaluation (with many metrics for all kinds of tasks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "revised-density",
   "metadata": {},
   "source": [
    "### 1.1. Data representation in scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gentle-corruption",
   "metadata": {},
   "source": [
    "In scikit-learn, data is represented in the same way it was in the previous exercises. That is:\n",
    "- The features are represented as a 2D features matrix (usually named `X`), most often contained in a NumPy array or Pandas DataFrame. \n",
    "- The label (or target) array is often called `y`, and is usually contained in a NumPy array or Pandas Series.\n",
    "\n",
    "In mathematical notation, this is:\n",
    "- features: $\\boldsymbol{X} \\in \\mathbb{R}^{N \\times D}$, $\\forall \\ \\boldsymbol{x}^{(i)} \\in \\boldsymbol{X}: \\boldsymbol{x}^{(i)} \\in \\mathbb{R}^{D}$\n",
    "- label (or target): $\\boldsymbol{y} \\in \\mathbb{R}^{N}$  \n",
    "where $N$ is the number of examples in our dataset, and $D$ is the number of features per example  \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "paperback-vinyl",
   "metadata": {},
   "source": [
    "scikit-learn offers many utilities for splitting and preprocessing data. \n",
    "- For splitting data, there are functions such as [`model_selection.train_test_split()`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html#sklearn.model_selection.train_test_split) which splits arrays or matrices into random train and test subsets, or [`model_selection.KFold()`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html#sklearn.model_selection.KFold) and similar functions which provides train/test indices for cross-validation. These functions are extremely handy, and are often used to split NumPy or Pandas arrays even when the training and models come from a library other than scikit-learn.\n",
    "- For preprocessing data, scikit-learn offers many utility functions which can standardize data (e.g. [`preprocessing.StandardScaler()`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler)), impute, discretize and perform feature expansion. For more informaton, refer to the [official preprocessing tutorial](https://scikit-learn.org/stable/modules/preprocessing.html#)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sensitive-blood",
   "metadata": {},
   "source": [
    "### 1.2. Estimator API\n",
    "\n",
    "\n",
    "For **supervised learning**, scikit-learn implements many algorithms we've seen in this class such as:\n",
    "- Nearest neighbors\n",
    "- Linear regression\n",
    "- Logistic regression\n",
    "- Support vector machines\n",
    "- Naive Bayes\n",
    "- Decision trees\n",
    "- Ensembles (such as random forests)\n",
    "\n",
    "In scikit-learn, these algorithms are called **estimators**, and they use a clean, uniform and streamlined API, which makes it very easy to switch to a new model or algorithm.\n",
    "\n",
    "Here is an example of many of the estimators available with scikit-learn. [Source](https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html)\n",
    "![](https://scikit-learn.org/stable/_images/sphx_glr_plot_classifier_comparison_001.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rural-crest",
   "metadata": {},
   "source": [
    "Here are the steps to follow when using the scikit-learn estimator API:\n",
    "1. Arrange data into a features matrix (`X`) and target vector (`y`).\n",
    "2. Choose a class of model by importing the appropriate estimator class (e.g. `linear_model.LogisticRegression()`, `svm.SVC()`, etc...)\n",
    "3. Choose model hyperparameters by instantiating this class with desired values.\n",
    "4. Fit the model to your data by calling the `fit()` method of the model instance.\n",
    "5. Apply the model to new data: for supervised learning, we predict labels for unknown data using the `predict()` method.\n",
    "\n",
    "The steps to follow when using scikit-learn estimators for unsupervised learning are almost identical."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extended-branch",
   "metadata": {},
   "source": [
    "### 1.3. Example: Logistic regression on the Iris dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liberal-starter",
   "metadata": {},
   "source": [
    "As an example, we'll walk through how to use scikit-learn to train a logistic regression model for multi-class classification the Iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legal-cameroon",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_theme(style=\"white\", context=\"notebook\", palette=\"dark\")\n",
    "# !!! sklearn is how the scikit-learn package is called in Python\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broken-viking",
   "metadata": {},
   "source": [
    "#### 1.3.1. Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "through-obligation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "# Iris is a toy dataset , which is directly available in sklearn.datasets\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "X = iris.data[:, :2] # we only take the first two features for simpler visualisations\n",
    "y = iris.target\n",
    "\n",
    "print(f\"Type of X: {type(X)} | Shape of X: {X.shape}\")\n",
    "print(f\"Type of y: {type(y)} | Shape of y: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lyric-minute",
   "metadata": {},
   "source": [
    "####  1.3.2. Splitting  and scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rising-mystery",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Split data using train_test_split, use 30% of the data as a test set and set a random state for reproducibility\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "print(f\"Shape of X_train: {X_train.shape} | Shape of y_train: {y_train.shape}\")\n",
    "print(f\"Shape of X_test: {X_test.shape} | Shape of y_test: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "separated-hawaiian",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "# Fit with the mean / std of the training data\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# Scale both the training / test data\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Mean of X_train: {X_train.mean():.3f}| Std of X_train: {X_train.std():.3f}\")\n",
    "print(f\"Mean of X_test: {X_test.mean():.3f}| Std of X_test: {X_test.std():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "original-stuff",
   "metadata": {},
   "source": [
    "#### 1.3.3. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "official-armor",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "# Initialize a logistic regression model with L2 regularization \n",
    "# and regularization strength 1e-4 (as C is inverse of regularization strength)\n",
    "logreg = LogisticRegression(penalty=\"l2\", C=1e4)\n",
    "\n",
    "# Train the model\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Get train accuracy\n",
    "train_acc = logreg.score(X_train, y_train)\n",
    "print(f\"Train accuracy: {train_acc * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brown-atlantic",
   "metadata": {},
   "source": [
    "#### 1.3.4. Decision boundaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plastic-crash",
   "metadata": {},
   "source": [
    "We can use matplotlib to view the decision boundaries of our trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "difficult-generator",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is beyond the scope of this class, no need to understand what it does.\n",
    "# Source: https://scikit-learn.org/stable/auto_examples/linear_model/plot_iris_logistic.html\n",
    "# Plot the decision boundary. For that, we will assign a color to each\n",
    "# point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "x_min, x_max = X_train[:, 0].min() - .5, X_train[:, 0].max() + .5\n",
    "y_min, y_max = X_train[:, 1].min() - .5, X_train[:, 1].max() + .5\n",
    "h = .02  # step size in the mesh\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "Z = logreg.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.figure()\n",
    "plt.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired, shading='auto', alpha=0.1, antialiased=True)\n",
    "\n",
    "# Plot also the training points\n",
    "scatter = plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, edgecolors='k', cmap=plt.cm.Paired)\n",
    "plt.xlabel('Sepal length')\n",
    "plt.ylabel('Sepal width')\n",
    "\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())\n",
    "plt.legend(handles=scatter.legend_elements()[0], labels=list(iris.target_names))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extensive-donna",
   "metadata": {},
   "source": [
    "#### 1.3.5. Test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pregnant-purpose",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get test accuracy\n",
    "test_acc = logreg.score(X_test, y_test)\n",
    "print(f\"Test accuracy: {test_acc * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "relevant-whale",
   "metadata": {},
   "source": [
    "#### 1.3.6. Other metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comic-franchise",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can easily use other metrics using sklearn.metrics\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "# First we'll use the balanced accuracy\n",
    "y_pred_train = logreg.predict(X_train)\n",
    "train_balanced_acc = balanced_accuracy_score(y_train, y_pred_train)\n",
    "\n",
    "y_pred_test = logreg.predict(X_test)\n",
    "test_balanced_acc = balanced_accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "print(f\"Train balanced acc: {train_balanced_acc*100:.2f}%\")\n",
    "print(f\"Test balanced acc: {test_balanced_acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generic-fishing",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "# Now we'll plot the confusion matrix of the testing data\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_test)\n",
    "ConfusionMatrixDisplay(conf_matrix, display_labels=iris.target_names).plot(cmap=plt.cm.Blues)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stuck-program",
   "metadata": {},
   "source": [
    "### 1.4. Additional scikit-learn resources\n",
    "\n",
    "This tutorial very briefly covers the scikit-learn package, and how it can be used to train a simple classifier. This package is capable of a lot more than what was shown here, as you will see in the rest of this exercise. If you want a more in-depth look at scikit-learn, take a look at these resources:\n",
    "\n",
    "- scikit-learn Getting Started tutorial: https://scikit-learn.org/stable/getting_started.html\n",
    "- scikit-learn User Guide: https://scikit-learn.org/stable/user_guide.html\n",
    "- scikit-learn cheatsheet by Datacamp: https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Scikit_Learn_Cheat_Sheet_Python.pdf\n",
    "- scikit-learn tutorial from the Python Data Science Handbook: https://jakevdp.github.io/PythonDataScienceHandbook/05.02-introducing-scikit-learn.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "active-riverside",
   "metadata": {},
   "source": [
    "## 2. Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "provincial-order",
   "metadata": {},
   "source": [
    "In class, we have covered the theory behind SVMs, and how they can be used to perform non-linear classification using the \"kernel trick\". In this exercise, you'll see how SVMs can easily be trained with scikit-learn, and how the choice of kernel can impact the performance on a non-linearly separable dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "labeled-visiting",
   "metadata": {},
   "source": [
    "### 2.1. Linear SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sapphire-feelings",
   "metadata": {},
   "source": [
    "First we'll show how to train a simple SVM classifier.\n",
    "In scikit-learn, the corresponding estimator is called `SVC` (Support Vector Classifier).\n",
    "\n",
    "In this part, we'll use a toy dataset which is linearly separable, generated using the `datasets.make_blobs()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effective-router",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import plot_svc_decision_function\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# Generate a linearly separable dataset\n",
    "X, y = make_blobs(n_samples=150, centers=2, random_state=0, cluster_std=0.70)\n",
    "# Split into train / test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Plot training and test data (color is for classes, shape is for train / test)\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, s=50, marker='o', cmap=\"viridis\", label=\"train\")\n",
    "plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, s=50, marker='^', cmap=\"viridis\", label=\"test\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fiscal-convertible",
   "metadata": {},
   "source": [
    "For this part, we'll train a SVM with a linear kernel. This corresponds to the basic SVM model that you've seen in class.\n",
    "\n",
    "When initializing an instance of the SVC class, you can specify a regularization parameter C, and the strength of regularization is inversely proportional to C.  That is, a high value of C leads to low regularization and a low C leads to high regularization. \n",
    "Try changing the value of C. How does it affect the support vectors? \n",
    "\n",
    "**Answer:**\n",
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "circular-quarter",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC # SVC = Support vector classifier\n",
    "\n",
    "# C is the regularization parameter. The strength of regularization is inversely proportional to C.\n",
    "# Try very large and very small values of C\n",
    "model = SVC(kernel='linear', C=1)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Print training accuracy\n",
    "train_acc = model.score(X_train, y_train)\n",
    "print(f\"Train accuracy: {train_acc * 100:.2f}%\")\n",
    "\n",
    "# Show decision function and support vectors\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, s=50, cmap=\"viridis\")\n",
    "plt.title(f\"Kernel = {model.kernel} | C = {model.C}\")\n",
    "plot_svc_decision_function(model, plot_support=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bound-longer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print test accuracy\n",
    "test_acc = model.score(X_test, y_test)\n",
    "print(f\"Test accuracy: {test_acc * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continuous-collectible",
   "metadata": {},
   "source": [
    "### 2.2. Kernel SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proud-amber",
   "metadata": {},
   "source": [
    "Let's now use a non-linearly separable dataset, to observe the effect of the kernel function in SVMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incredible-school",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "\n",
    "# Generate a circular dataset\n",
    "X, y = make_circles(n_samples=400, noise=0.25, factor=0, random_state=0)\n",
    "# Split into train / test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# Plot training and test data (color is for classes, shape is for train / test)\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, marker='o', cmap=\"viridis\", label=\"train\")\n",
    "plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, marker='^', cmap=\"viridis\", label=\"test\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elder-sampling",
   "metadata": {},
   "source": [
    "As you've seen in class, we can use kernel functions to allow SVMs to operate in high-dimensional, implicit feature spaces, without needing to compute the coordinates of the data in that space. We have seen a variety of kernel functions, such as the polynomial kernel and the RBF kernel.\n",
    "\n",
    "In this exercise, experiment with the different kernels, such as:\n",
    "- the linear kernel (`linear`): $\\langle x, x'\\rangle$\n",
    "- the polynomial kernel (`poly`): $(\\gamma \\langle x, x'\\rangle + r)^d$ (try out different degrees)\n",
    "- the radial basis function kernel (`rbf`): $\\exp(-\\gamma \\|x-x'\\|^2)$\n",
    "\n",
    "Your task is to experiment with these kernels to see which one does the best on this dataset. \n",
    "\n",
    "How does the kernel affect the decision boundary? Which kernel and value of C would you pick to maximize your model's performance? \n",
    "\n",
    "**Note:** Use the the helper function `plot_svc_decision_function()` to view the decision boundaries for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complimentary-royal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use as many code cells as needed to try out different kernels and values of C\n",
    "### YOUR CODE HERE ###\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "announced-geography",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "republican-lingerie",
   "metadata": {},
   "source": [
    "**To go further**: To learn more about SVMs in scikit-learn, and how to use them for multi-class classification and regression, check out the documentation page: https://scikit-learn.org/stable/modules/svm.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rental-backing",
   "metadata": {},
   "source": [
    "## 3. Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "devoted-tourist",
   "metadata": {},
   "source": [
    "Decision trees are a very intuitive way to classify objects: they ask a series of questions to infer the target variable. \n",
    "\n",
    "A decision tree is a set of nested decision rules. At each node $i$, the $d_i$-th feature of the input vector $ \\boldsymbol{x}$ is compared to a treshold value $t$. The vector $\\boldsymbol{x}$ is passed down to the left or right branch depending on whether $d_i$ is less than or greater than $t$. This process is repeated for each node encountered until a reaching leaf node, which specifies the predicted output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bright-collect",
   "metadata": {},
   "source": [
    "<img src=\"images/simple_tree.png\" width=400></img>\n",
    "\n",
    "*Example of a simple decision tree on the Palmer Penguins dataset*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verified-nickname",
   "metadata": {},
   "source": [
    "Decision trees are usually constructed from the top-down, by choosing a feature at each step that best splits the set of items. There are different metrics for measuring the \"best\" feature to pick, such as the Gini impurity and the entropy / information gain. We won't dive into them here, but we recommend reading Chapter 18 of [\"Probabilistic Machine Learning: An Introduction\"](https://probml.github.io/pml-book/) by K.P. Murphy if you want to learn more about them.\n",
    "\n",
    "Decision trees are popular for several reasons:\n",
    "- They are **easy to interpret**.\n",
    "- They can handle mixed discrete and continuous inputs.\n",
    "- They are insensitive to monotone transformations of the inputs, so there is no need to standardize the data.\n",
    "- They perform automatic feature selection.\n",
    "- They are fast to Ô¨Åt, and scale well to large data sets.\n",
    "\n",
    "Unfortunately, trees usually do not predict as accurately as other models we have seen previously, such as neural networks and SVMs.\n",
    "\n",
    "It is however possible to significantly improve their performance through an ensemble learning method called **random forests**, which consists of constructing a multitude of decision trees at training time and averaging their outputs at test time. While random forests usually perform better than a single decision tree, they are much less interpretable. We won't cover random forests in this exercise, but keep in mind that they can be easily implemented in scikit-learn using the [`ensemble` module](https://scikit-learn.org/stable/modules/ensemble.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prepared-johnson",
   "metadata": {},
   "source": [
    "### 3.1. Training  decision trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alive-office",
   "metadata": {},
   "source": [
    "In this part, we will work on the Titanic dataset obtained at the end of the `05-pandas` tutorial. Our goal is to train a model to predict whether or not a passenger survived the shipwreck and to find out which features are the most useful for predicting this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adult-allen",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "titanic = pd.read_csv(\"data/titanic.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subjective-slide",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tender-fisher",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into X and y\n",
    "X = titanic.drop(columns=\"survived\")\n",
    "y = titanic[\"survived\"]\n",
    "# Convert to NumPy (needed for interpretability function later on)\n",
    "X_numpy, y_numpy = X.to_numpy(), y.to_numpy()\n",
    "\n",
    "# Use 80% of data for train/val, 20% for test\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(X_numpy, y_numpy, test_size=0.2, random_state=42)\n",
    "# Use 80% of trainval for train, 20% for val\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_trainval, y_trainval, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "marine-surprise",
   "metadata": {},
   "source": [
    "It is now your turn to train decision trees in scikit-learn. They follow the same estimator API as all other supervised learning models, so the implementation is very straightforward. For more information, check out the [`tree` module](https://scikit-learn.org/stable/modules/tree.html#tree).\n",
    "\n",
    "**Your task:** Initialize a `DecisionTreeClassifier` and train it on `X_train` and `y_train`. \n",
    "- Use \"entropy\" as the `criterion`\n",
    "- Try out different values for the max tree depth. How does it affect the train and validation accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "graduate-electron",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.tree as tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Use the entropy (information gain) as the criterion\n",
    "# Try varying the max depth\n",
    "\n",
    "### YOUR CODE HERE ###\n",
    "model = ...\n",
    "### END CODE HERE ###\n",
    "\n",
    "train_acc = model.score(X_train, y_train)\n",
    "print(f\"Train accuracy: {train_acc * 100:.2f}%\")\n",
    "\n",
    "val_acc = model.score(X_val, y_val)\n",
    "print(f\"Validation accuracy: {val_acc * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fluid-seven",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc = model.score(X_test, y_test)\n",
    "print(f\"Test accuracy: {test_acc * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bronze-programming",
   "metadata": {},
   "source": [
    "**Answer:** YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "geological-plymouth",
   "metadata": {},
   "source": [
    "### 3.2. Interpretability of trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "theoretical-sociology",
   "metadata": {},
   "source": [
    "In this section, we'll show you how to visualize decision trees and interpret the decision made for some examples of our test set.\n",
    "\n",
    "**Your task:** Run the next few cells to better understand the structure of the tree you just built. Can you identify which features are the most important for predicting whether or not a passenger survived?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "talented-rover",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots the decision tree\n",
    "# Try out a max plot depth of 2 or 3, tree will be hard to read otherwise\n",
    "plt.figure(figsize=(30, 10))\n",
    "tree.plot_tree(model,  max_depth=2, filled=True, feature_names=X.columns, class_names=[\"Perished\", \"Survived\"], \n",
    "               impurity=False, proportion=True, rounded=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "architectural-scene",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prints the decision tree as text\n",
    "# Will be very long if max depth is high\n",
    "# Class 0 = Perished, Class 1 = Survived\n",
    "print(tree.export_text(model, feature_names=list(X.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bottom-punch",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explaining the decisions (complicated code, no need to understand what it does exactly)\n",
    "def explain_decision(sample_id: int = 0):\n",
    "    \"\"\"Prints rules followed to obtain prediction for a sample of the test set\n",
    "    \n",
    "    Code adapted from: \n",
    "    https://scikit-learn.org/stable/auto_examples/tree/plot_unveil_tree_structure.html\n",
    "    \"\"\"\n",
    "    sample_id = sample_id\n",
    "    class_names=[\"Perished\", \"Survived\"]\n",
    "    n_nodes = model.tree_.node_count\n",
    "    children_left = model.tree_.children_left\n",
    "    children_right = model.tree_.children_right\n",
    "    feature = model.tree_.feature\n",
    "    threshold = model.tree_.threshold\n",
    "\n",
    "    node_indicator = model.decision_path(X_test)\n",
    "    leaf_id = model.apply(X_test)\n",
    "    # obtain ids of the nodes `sample_id` goes through, i.e., row `sample_id`\n",
    "    node_index = node_indicator.indices[node_indicator.indptr[sample_id]:\n",
    "                                        node_indicator.indptr[sample_id + 1]]\n",
    "\n",
    "    prediction = class_names[model.predict(X_test[sample_id:sample_id+1])[0]]\n",
    "    print(f\"Prediction for sample {sample_id}: {prediction}\\n\")\n",
    "    print(\"Rules used:\")\n",
    "    for node_id in node_index:\n",
    "        # continue to the next node if it is a leaf node\n",
    "        if leaf_id[sample_id] == node_id:\n",
    "            continue\n",
    "\n",
    "        # check if value of the split feature for sample 0 is below threshold\n",
    "        if (X_test[sample_id, feature[node_id]] <= threshold[node_id]):\n",
    "            threshold_sign = \"<=\"\n",
    "        else:\n",
    "            threshold_sign = \">\"\n",
    "\n",
    "        print(\"- node {node}: ({feature} = {value}) \"\n",
    "              \"{inequality} {threshold}\".format(\n",
    "                  node=node_id,\n",
    "                  feature=X.columns[feature[node_id]],\n",
    "                  value=X_test[sample_id, feature[node_id]],\n",
    "                  inequality=threshold_sign,\n",
    "                  threshold=threshold[node_id]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incident-liver",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For binary variables, 1 = True, 0 = False\n",
    "# e.g. sex_male = 1 -> male, sex_male = 0 -> female\n",
    "# Many of the features are redundant (e.g. sex_male and sex_female)\n",
    "# so the tree doesn't always choose the same features \n",
    "\n",
    "explain_decision(sample_id=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phantom-morrison",
   "metadata": {},
   "outputs": [],
   "source": [
    "explain_decision(sample_id=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "italian-stranger",
   "metadata": {},
   "source": [
    "**Answer:** \n",
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blond-youth",
   "metadata": {},
   "source": [
    "**To go further:** Decision trees and random forests can also be used for regression, check out the scikit-learn pages on [trees](https://scikit-learn.org/stable/modules/tree.html#tree) and [ensemble methods](https://scikit-learn.org/stable/modules/ensemble.html#ensemble) for more info."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bound-backing",
   "metadata": {},
   "source": [
    "## 4. A small note on unsupervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sustained-cholesterol",
   "metadata": {},
   "source": [
    "While we won't cover them in this exercise, most of the unsupervised learning techniques seen in class can be easily implemented with scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fleet-gathering",
   "metadata": {},
   "source": [
    "As an example, here is how to use the k-means clustering algorithm on a toy dataset consisting of 7 unlabeled blobs of points. \n",
    "When choosing $k=7$, k-means manages to almost perfectly recover the original blobs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "several-mounting",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Generate unlabeled data\n",
    "X_blobs, _ = make_blobs(n_samples=200, centers=7, random_state=0, cluster_std=0.60)\n",
    "plt.scatter(X_blobs[:, 0], X_blobs[:, 1], c=\"grey\", alpha=0.5)\n",
    "plt.title(\"Unlabeled data\")\n",
    "plt.show()\n",
    "\n",
    "# Run k-means on data to find the blobs\n",
    "\n",
    "# Try changing the value of k\n",
    "k = 7\n",
    "kmeans = KMeans(n_clusters=k)\n",
    "labels = kmeans.fit_predict(X_blobs)\n",
    "\n",
    "# Display clusters and their centers\n",
    "plt.scatter(X_blobs[:,0], X_blobs[:,1], c=labels, cmap=\"viridis\", alpha=0.5)\n",
    "for c in kmeans.cluster_centers_:\n",
    "        plt.scatter(c[0], c[1], marker=\"*\", s=80, color=\"blue\")\n",
    "plt.title(f\"K-Means with {k} clusters\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respiratory-proxy",
   "metadata": {},
   "source": [
    "To learn more about how to practically implement these techniques, check out these resources:\n",
    "\n",
    "**For dimensionality reduction:**\n",
    "- [PCA from the Python Data Science handbook](https://jakevdp.github.io/PythonDataScienceHandbook/05.09-principal-component-analysis.html)\n",
    "- [Manifold learning from the Python Data Science handbook](https://jakevdp.github.io/PythonDataScienceHandbook/05.10-manifold-learning.html)\n",
    "- [Decomposition page on scikit-learn's website](https://scikit-learn.org/stable/modules/decomposition.html)\n",
    "- [Manifold learning page on scikit-learn's website](https://scikit-learn.org/stable/modules/manifold.html)\n",
    "\n",
    "**For clustering:**\n",
    "- [k-means from the Python Data Science handbook](https://jakevdp.github.io/PythonDataScienceHandbook/05.11-k-means.html)\n",
    "- [Gaussian mixtures from the Python Data Science handbook](https://jakevdp.github.io/PythonDataScienceHandbook/05.12-gaussian-mixtures.html)\n",
    "- [Clustering page on scikit-learn's website](https://scikit-learn.org/stable/modules/clustering.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loved-reform",
   "metadata": {},
   "source": [
    "## Congratulations!\n",
    "\n",
    "Congratulations on completing this final exercise!\n",
    "\n",
    "Throughout this series of exercises, you learned about the fundamental tools and libraries used in machine learning, and worked on practical implementations of many of the most commonly used techniques in this field. \n",
    "\n",
    "As long as these exercises have been, they are still too short to cover several other interesting and important machine learning topics, but we believe you now have all the tools at your disposal to learn about them on your own, if you desire to do so.  \n",
    "\n",
    "Thank you for sticking with us through the end, we really hope you enjoyed the exercises in this course!\n",
    "\n",
    "<img src=\"images/thats_all_folks.png\" width=400></img>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
