{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fifteen-convertible",
   "metadata": {},
   "source": [
    "# Neural networks with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cathedral-lover",
   "metadata": {},
   "source": [
    "<hr style=\"clear:both\">\n",
    "\n",
    "This notebook is part of a series of exercises for the CIVIL-226 Introduction to Machine Learning for Engineers course at EPFL. Copyright (c) 2021 [VITA](https://www.epfl.ch/labs/vita/) lab at EPFL  \n",
    "Use of this source code is governed by an MIT-style license that can be found in the LICENSE file or at https://www.opensource.org/licenses/MIT\n",
    "\n",
    "**Author(s):** [David Mizrahi](mailto:david.mizrahi@epfl.ch)\n",
    "<hr style=\"clear:both\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "living-marathon",
   "metadata": {},
   "source": [
    "In this exercise, we'll cover the basics of the [PyTorch](https://pytorch.org) package and use it to implement a simple neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adaptive-producer",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bulgarian-spoke",
   "metadata": {},
   "source": [
    "If you're running this notebook locally, you'll first need to install PyTorch and torchvision. You can do so by following the instructions at: https://pytorch.org/get-started/locally/. Make sure to install these packages in the `introml` environment created for this course.\n",
    "\n",
    "Reminder:\n",
    "\n",
    "0. Use anaconda prompt on windows or terminal on mac/linux\n",
    "1. Activate the `introml` environment: `conda activate introml`\n",
    "2. Copy paste the command provided in the [link](https://pytorch.org/get-started/locally/) in your terminal\n",
    "3. Go to the directory where this notebook is located: `cd path/to/notebooks`\n",
    "4. Start Jupyter Notebook: `jupyter notebook`\n",
    "\n",
    "If you're using Google Colab or EPFL Noto, PyTorch should already be installed. You can check by trying to import the package with `import torch`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "particular-inspector",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that PyTorch and torchvision are installed correctly\n",
    "# Note: You may need to restart your kernel and re-run this cell before running the following cells\n",
    "\n",
    "import torch\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recognized-december",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spoken-novelty",
   "metadata": {},
   "outputs": [],
   "source": [
    "torchvision.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "million-season",
   "metadata": {},
   "source": [
    "#### For Google Colab\n",
    "You can run this notebook in Google Colab using the following link: https://colab.research.google.com/github/vita-epfl/introML-2021/blob/main/exercises/06-neural-nets/neural_nets_pytorch.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "announced-evening",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  import google.colab\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False\n",
    "if IN_COLAB:\n",
    "  # Clone the entire repo to access the files\n",
    "  !git clone -l -s https://github.com/vita-epfl/introML-2021.git cloned-repo\n",
    "  %cd cloned-repo/exercises/06-neural-nets/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spoken-glucose",
   "metadata": {},
   "source": [
    "# Exercise 1: Deep Learning with PyTorch: A 60 Minute Blitz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unauthorized-capital",
   "metadata": {},
   "source": [
    "PyTorch's website offers an excellent tutorial on the basics of PyTorch, which can be found  at the following link: https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html\n",
    "\n",
    "We heavily recommend completing this tutorial before moving on to the second exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "welcome-speaker",
   "metadata": {},
   "source": [
    "# Exercise 2: Simple neural nets with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outer-terry",
   "metadata": {},
   "source": [
    "In this exercise, we'll implement classifiers with PyTorch.\n",
    "More specifically, we'll compare a simple logistic regression model to a simple neural network on the MNIST dataset.\n",
    "\n",
    "Here is the general pipeline used to train neural networks with PyTorch:\n",
    "\n",
    "```\n",
    "1. Load the dataset\n",
    "2. Initialize a dataloader, define data transforms\n",
    "3. Define and instantiate network architecture\n",
    "4. Choose a loss function\n",
    "5. Choose an optimizer (incl. learning rate, weight decay & momentum)\n",
    "6. Define the training loop & number of epochs\n",
    "7. Train the model\n",
    "8. Check validation accuracy, visualize results, adjust hyper-parameters (not done in this exercise)\n",
    "9. Repeat steps 2-9 until satisfied with validation accuracy (not done in this exercise)\n",
    "10. Check test accuracy\n",
    "```\n",
    "\n",
    "*Epoch: number of times that the learning algorithm will work through the entire training set*\n",
    "\n",
    "In this exercise, we'll follow these steps to implement our classifiers. Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finnish-young",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "promotional-leeds",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch & torchvision\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import MNIST, FashionMNIST\n",
    "\n",
    "# Progress bar\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Helper files\n",
    "import helpers\n",
    "import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compressed-insight",
   "metadata": {},
   "source": [
    "Here is a brief description of these imported packages:\n",
    "\n",
    "**PyTorch:**\n",
    "- `torch.nn` Contains the basic building blocks to implement neural nets (incl. different types of layers and loss functions) | [Documentation](https://pytorch.org/docs/stable/nn.html)\n",
    "- `torch.nn.functional` A functional (stateless) approach to torch.nn, often used for stateless objects (e.g. ReLU) | [Documentation](https://pytorch.org/docs/stable/nn.functional.html) | [More info](https://discuss.pytorch.org/t/what-is-the-difference-between-torch-nn-and-torch-nn-functional/33597/2)\n",
    "- `torch.optim` A package implementing various optimization algorithms, such as SGD and Adam | [Documentation](https://pytorch.org/docs/stable/optim.html)\n",
    "\n",
    "**torchvision:**\n",
    "- `torchvision.transforms` Common image transformations\n",
    "- `torchvision.datasets` Popular image datasets\n",
    "\n",
    "**`tqdm`:** Popular package used to show progress bars | [Documentation](https://tqdm.github.io/)\n",
    "\n",
    "**`helpers`**: Contains functions to help visualize data and predictions\n",
    "\n",
    "**`metrics`:** Contains two simple classes that help keep track and compute the loss and accuracy over a training epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "empirical-voluntary",
   "metadata": {},
   "source": [
    "## 2. Loading and visualizing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "transparent-intake",
   "metadata": {},
   "source": [
    "### 2.1. Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "political-paraguay",
   "metadata": {},
   "source": [
    "Here, we'll use the classic [MNIST](http://yann.lecun.com/exdb/mnist/) dataset, which consists of black-and-white 28x28  images of hand-drawn digits (between 0 and 9). \n",
    "The training set has 60,000 examples, and the test set 10,000 examples.\n",
    "\n",
    "The `torchvision` package provides an easy way to load this dataset (among many others). \n",
    "\n",
    "A list of datasets that can be loaded this way can be found at: https://pytorch.org/vision/stable/datasets.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "restricted-handbook",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataset in a folder called \"/data\"\n",
    "root = \"data/\"\n",
    "\n",
    "# transforms.ToTensor() is used to convert the downloaded PIL Image to a torch Tensor\n",
    "train_data = MNIST(root, train=True, transform=transforms.ToTensor(), download=True)\n",
    "test_data = MNIST(root, train=False, transform=transforms.ToTensor(), download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "royal-memphis",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Images in training data: {len(train_data)}\")\n",
    "print(f\"Images in test data: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conventional-victor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the mapping from target value to class name (if you're using MNIST, you won't be too surprised)\n",
    "{i: class_name for i, class_name in enumerate(train_data.classes)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "referenced-technician",
   "metadata": {},
   "source": [
    "### 2.2. Dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numeric-activity",
   "metadata": {},
   "source": [
    "When training neural networks, we usually use mini-batches of data for each forward + backward pass. \n",
    "\n",
    "In order to obtain those mini-batches, we must pass our dataset through `torch.utils.DataLoader`, which combines the dataset and a sampler, and returns an iterable over the data and labels of our dataset.\n",
    "\n",
    "In this exercise, we'll pick a batch size of 32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "possible-dutch",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "# Reshuffle training data at every epoch, but not the test data \n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "important-beverage",
   "metadata": {},
   "source": [
    "Let's take a look at one of our batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "patient-creativity",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, targets = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frank-africa",
   "metadata": {},
   "outputs": [],
   "source": [
    "helpers.imshow(torchvision.utils.make_grid(images, nrow=8))\n",
    "print(targets.reshape(-1, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prerequisite-metadata",
   "metadata": {},
   "source": [
    "## 3. Simple model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ongoing-wheat",
   "metadata": {},
   "source": [
    "First, we'll build a simple classifier (akin to logistic regression)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "violent-harvard",
   "metadata": {},
   "source": [
    "###  3.1 Network architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "monetary-style",
   "metadata": {},
   "source": [
    "#### Short primer on `nn.Module`\n",
    "\n",
    "In PyTorch, each neural net architecture is a subclass of `nn.Module` ([Documentation](https://pytorch.org/docs/stable/generated/torch.nn.Module.html))\n",
    "\n",
    "To quote an [official tutorial](https://pytorch.org/tutorials/beginner/nlp/deep_learning_tutorial.html?highlight=module):\n",
    "\n",
    "> All network components should inherit from `nn.Module` and override the `forward()` method. That is about it, as far as the boilerplate is concerned. Inheriting from `nn.Module` provides functionality to your component. For example, it makes it keep track of its trainable parameters, you can swap it between CPU and GPU with the `.to(device)` method, where device can be a CPU device `torch.device(\"cpu\")` or CUDA device `torch.device(\"cuda:0\")`.\n",
    "\n",
    "\n",
    "In order to implement your model, you'll therefore need to fill in two methods:\n",
    "\n",
    "**`__init__()`**: \n",
    "\n",
    "- Initialize your layers here, so that the module can keep track of these layers' parameters. \n",
    "- It is not necessary to initialize layers with no learnable parameters (e.g. `ReLU`), as you can use  the`nn.functional` API for those if you want.\n",
    "\n",
    "**`forward()`**:\n",
    "\n",
    "- Define your model architecture here (i.e. call your layers in the desired order). You can use any of the Tensor operations in the `forward` function.\n",
    "- This function defines the computation performed at every call. The backward function (where gradients are computed) is automatically defined for you using autograd. \n",
    "\n",
    "The learnable parameters of a model are returned by `model.parameters()`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "retired-alberta",
   "metadata": {},
   "source": [
    "#### One layer neural net\n",
    "\n",
    "You should implement a one-layer network composed of the following:\n",
    "- A layer that flattens the image (already implemented)\n",
    "- A fully-connected layer from the flattened input of shape (784,) to the output layer of shape (10,) as we have 10 different classes.\n",
    "\n",
    "This one-layer neural network is equivalent to softmax regression. \n",
    "\n",
    "You may wonder why there is no softmax layer in this architecture. This is because it is directly added to the [`CrossEntropyLoss()` module](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html), which expects raw, unnormalized scores for each class.\n",
    "\n",
    "As a result, we also added a `predict()` method that adds this softmax layer, it'll be useful later on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "muslim-sector",
   "metadata": {},
   "source": [
    "**One layer neural net:**\n",
    "\n",
    "<img src=\"images/1_layer_net.png\" width=220></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "given-salad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneLayerNet(nn.Module):\n",
    "    \"\"\"1-Layer MNIST classifier\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        ### START CODE HERE ###\n",
    "        \n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Flatten to get tensor of shape (batch_size, 784)\n",
    "        x = x.flatten(start_dim=1)\n",
    "        ### START CODE HERE ###\n",
    "        \n",
    "        \n",
    "        return ...\n",
    "        ### END CODE HERE ###\n",
    "  \n",
    "    def predict(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Predicts classes by calculating the softmax\"\"\"\n",
    "        logits = self.forward(x)\n",
    "        return F.softmax(logits, dim=1)\n",
    "\n",
    "model = OneLayerNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exceptional-binding",
   "metadata": {},
   "source": [
    "**Question:** How many trainable parameters (weights) does this network have?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afraid-boulder",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "YOUR ANSWER HERE\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "norman-water",
   "metadata": {},
   "source": [
    "### 3.2. Loss function and optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assured-syndrome",
   "metadata": {},
   "source": [
    "We'll keep things simple here, and choose Cross Entropy loss as our loss function, and Stochastic Gradient Descent (SGD) with a learning rate of 0.05 as our optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instructional-harvest",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alone-training",
   "metadata": {},
   "source": [
    "### 3.3. Training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "czech-snake",
   "metadata": {},
   "source": [
    "The training loop can be defined as follows:\n",
    "\n",
    "```\n",
    "For each batch in the dataset:\n",
    "   1. Load the batch\n",
    "   2. Zero-out the accumulated gradients (PyTorch doesn't clear them automatically).\n",
    "   3. Run the forward pass through your model.\n",
    "   4. Compute the loss.\n",
    "   5. Run the backward pass, i.e. compute gradients of the loss w.r.t. to the weights.\n",
    "   6. Update the weights using the optimizer.\n",
    "```\n",
    "\n",
    "Complete the training loop code of the `train()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lightweight-reform",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: torch.nn.Module, train_loader: torch.utils.data.DataLoader, loss_fn: torch.nn.Module, optimizer: torch.optim.Optimizer, epochs: int):\n",
    "    \n",
    "    # Initialize metrics for loss and accuracy\n",
    "    loss_metric = metrics.LossMetric()\n",
    "    acc_metric = metrics.AccuracyMetric(k=1)\n",
    "    \n",
    "    # Sets the module in training mode (doesn't have any effect here, but good habit to take)\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(1, epochs + 1):\n",
    "        \n",
    "        # Progress bar set-up\n",
    "        pbar = tqdm(total=len(train_loader), leave=True)\n",
    "        pbar.set_description(f\"Epoch {epoch}\")\n",
    "        \n",
    "        # Iterate through data\n",
    "        for data, target in train_loader:\n",
    "            \n",
    "            ### START CODE HERE ###\n",
    "            \n",
    "            # Zero-out the gradients\n",
    "            ...\n",
    "            \n",
    "            # Forward pass\n",
    "            out = ...\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = ...\n",
    "            \n",
    "            # Backward pass\n",
    "            ...\n",
    "            \n",
    "            # Optimizer step\n",
    "            ...\n",
    "            \n",
    "            ### END CODE HERE ###\n",
    "            \n",
    "            # Update metrics & progress bar\n",
    "            loss_metric.update(loss.item(), data.shape[0])\n",
    "            acc_metric.update(out, target)\n",
    "            pbar.update()\n",
    "            \n",
    "        # End of epoch, show loss and acc\n",
    "        pbar.set_postfix_str(f\"Train loss: {loss_metric.compute():.3f} | Train acc: {acc_metric.compute() * 100:.2f}%\")\n",
    "        loss_metric.reset()\n",
    "        acc_metric.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regional-christianity",
   "metadata": {},
   "source": [
    "Now that the train function is defined, it's time to actually train your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "editorial-snapshot",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model for 10 epochs\n",
    "train(model, train_loader, loss_fn, optimizer, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "homeless-disclaimer",
   "metadata": {},
   "source": [
    "### 3.4. Test accuracy\n",
    "\n",
    "You should now have a model with > 90% train accuracy, but this doesn't tell the whole story. In order to actually estimate how good our model is, we need to check its accuracy on the test set.\n",
    "\n",
    "Complete the function `test()` which computes the test accuracy of a given model on a specific dataset.\n",
    "\n",
    "This function iterates through a dataset once (using a DataLoader) and displays the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loose-mirror",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model: torch.nn.Module, dataloader: torch.utils.data.DataLoader):\n",
    "    \n",
    "    # Initialize accuracy metric\n",
    "    acc_metric = metrics.AccuracyMetric(k=1)\n",
    "    \n",
    "    # Progress bar set-up\n",
    "    pbar = tqdm(total=len(test_loader), leave=True)\n",
    "    \n",
    "    # Sets the module in eval mode (doesn't have any effect here, but good habit to take)\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        ### START CODE HERE ###\n",
    "        \n",
    "        # Iterate through data\n",
    "        for ... in ...:\n",
    "            \n",
    "            # Forward pass\n",
    "            ...\n",
    "            \n",
    "            # Update accuracy metric\n",
    "            ...\n",
    "\n",
    "            # Update progress bar\n",
    "            ...\n",
    "            \n",
    "        ### END CODE HERE ###\n",
    "            \n",
    "    # End of epoch, show loss and acc\n",
    "    test_acc = acc_metric.compute() * 100\n",
    "    pbar.set_postfix_str(f\"Acc: {test_acc:.2f}%\")\n",
    "    print(f\"Accuracy is {test_acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seeing-gathering",
   "metadata": {},
   "outputs": [],
   "source": [
    "test(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electronic-prediction",
   "metadata": {},
   "source": [
    "**Expected result:** >90% test accuracy on MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rotary-chaos",
   "metadata": {},
   "source": [
    "We obtain an accuracy of over 90% with this simple softmax regression model. A random classifier would only get an accuracy of ~10%, so this is quite an improvement!\n",
    "\n",
    "But let's not stop there, we can do even better with just a few small changes. Let's see how!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "three-syria",
   "metadata": {},
   "source": [
    "## 4. Simple neural net (with hidden layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "perceived-middle",
   "metadata": {},
   "source": [
    "Time to implement (actual) neural networks. As you'll soon see, the entire pipeline is very similar, only the network architecture changes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "herbal-responsibility",
   "metadata": {},
   "source": [
    "#### Network architecture\n",
    "\n",
    "You should implement a three-layer fully-connected neural net (2 hidden layers) composed of the following:\n",
    "- A layer that flattens the image (already implemented)\n",
    "- A fully-connected layer from the flattened input of shape (784,) to the first hidden layer of shape (100, ), with ReLU as an activation.\n",
    "- A fully-connected layer from the first hidden layer of shape (100,) to the second hidden layer of shape (100, ), with ReLU as an activation.\n",
    "- A fully-connected layer from the second hidden layer of shape (100,) to the output layer of shape (10,).\n",
    "\n",
    "**Note:** Fully connected neural networks (FCNNs) are sometimes also referred to as multilayer perceptrons (MLP) in ML literature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elect-piano",
   "metadata": {},
   "source": [
    "**Three-layer neural net:**\n",
    "\n",
    "<img src=\"images/3_layer_net.png\" width=250></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hollow-buying",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThreeLayerNet(nn.Module):\n",
    "    \"\"\"3-Layer neural net\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        ### START CODE HERE ###\n",
    "        \n",
    "        \n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Flatten to get tensor of shape (batch_size, 784)\n",
    "        x = x.flatten(start_dim=1)\n",
    "        ### START CODE HERE ###\n",
    "        \n",
    "        \n",
    "        \n",
    "        return ...\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "    def predict(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Predicts classes by calculating the softmax\"\"\"\n",
    "        logits = self.forward(x)\n",
    "        return F.softmax(logits, dim=1)\n",
    "\n",
    "model = ThreeLayerNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sunset-mistake",
   "metadata": {},
   "source": [
    "**Question:** How many trainable parameters (weights) does this network have?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "driven-gather",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "YOUR ANSWER HERE\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "center-kelly",
   "metadata": {},
   "source": [
    "#### Loss & optimizer\n",
    "\n",
    "Use the same loss function and optimizer as in the previous section. Namely, the Cross-Entropy loss and SGD with lr=0.05."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "employed-jackson",
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "loss_fn = ...\n",
    "optimizer = ...\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extensive-checklist",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "correct-shopper",
   "metadata": {},
   "source": [
    "Use `train()` to launch the training process, use the same number of epochs as in the previous part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ready-thinking",
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "...\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incomplete-stable",
   "metadata": {},
   "source": [
    "#### Test accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cutting-victorian",
   "metadata": {},
   "source": [
    "Compute the test accuracy using `test()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fewer-conditioning",
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "...\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "insured-welcome",
   "metadata": {},
   "source": [
    "**Expected result:** >96% test accuracy on MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sublime-chosen",
   "metadata": {},
   "source": [
    "## 5. Visualizing predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latter-personal",
   "metadata": {},
   "source": [
    "Let's take a look at a few of our model's predictions.\n",
    "\n",
    "First, we'll get the predictions for one batch of our test data (this is where the `predict()` method becomes useful)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clean-transportation",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, _ = next(iter(test_loader))\n",
    "preds = model.predict(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rental-imaging",
   "metadata": {},
   "source": [
    "Then, we'll use a function in `helpers` to view the classifier's softmax score next to each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flexible-crash",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shows the image next to the classifier's softmax score\n",
    "# Show for the first 10 images (change value to see more images)\n",
    "for i in range(10):\n",
    "    helpers.view_prediction(images[i], preds[i], test_data.classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extra-chicago",
   "metadata": {},
   "source": [
    "Not bad, right?\n",
    "\n",
    "Congratulations on finishing this exercise! If you want to try out your network on a different dataset, take a look at part 6.\n",
    "\n",
    "Next week, we'll take a look at convolutional neural networks, which can do even better than these fully-connected neural networks on image datasets. See you then!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "victorian-while",
   "metadata": {},
   "source": [
    "## 6. (Optional) Training your model on a different dataset\n",
    "There are many datasets designed to serve as a direct drop-in replacement to MNIST, one of them being [Fashion-MNIST](https://github.com/zalandoresearch/fashion-mnist), by Zalando Research.\n",
    "\n",
    "You can try out these two networks on Fashion-MNIST by just replacing the following two lines at the beginning of the notebook and re-running the entire code.\n",
    "\n",
    "Replace:\n",
    "```python\n",
    "train_data = MNIST(root, train=True, transform=transforms.ToTensor(), download=True)\n",
    "test_data = MNIST(root, train=False, transform=transforms.ToTensor(), download=True)\n",
    "```\n",
    "\n",
    "with:\n",
    "```python\n",
    "train_data = FashionMNIST(root, train=True, transform=transforms.ToTensor(), download=True)\n",
    "test_data = FashionMNIST(root, train=False, transform=transforms.ToTensor(), download=True)\n",
    "```\n",
    "\n",
    "**Note:** FashionMNIST is a harder dataset than MNIST, so your accuracy will likely be lower."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "theoretical-taiwan",
   "metadata": {},
   "source": [
    "## Additional PyTorch resources\n",
    "- PyTorch cheat sheet: https://pytorch.org/tutorials/beginner/ptcheat.html\n",
    "- Other PyTorch tutorials: https://pytorch.org/tutorials/index.html\n",
    "- PyTorch recipes: https://pytorch.org/tutorials/recipes/recipes_index.html (bite-sized code examples on specific PyTorch features)\n",
    "- PyTorch examples: https://github.com/pytorch/examples"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
