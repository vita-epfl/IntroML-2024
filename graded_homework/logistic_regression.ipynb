{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "8068dda051284404978b8f28c36914de",
    "deepnote_cell_type": "markdown",
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "38d358b2edfa7258c44bf997454910cc",
     "grade": false,
     "grade_id": "cell-a3edf79fa89d828d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Homework - Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "1e4e31068da74cb080185202093d51c7",
    "deepnote_cell_type": "markdown",
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "38eaa5311f559ee847f306c9c2ef2dee",
     "grade": false,
     "grade_id": "cell-dd226607fd592d7a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<hr style=\"clear:both\">\n",
    "\n",
    "This homework is part of a series of exercises for the CIVIL-226 Introduction to Machine Learning for Engineers course at EPFL. Copyright (c) 2023 [VITA](https://www.epfl.ch/labs/vita/) lab at EPFL  \n",
    "Use of this source code is governed by an MIT-style license that can be found in the LICENSE file or at https://www.opensource.org/licenses/MIT\n",
    "\n",
    "\n",
    "**2024 Revision:** [Bastien Van Delft](mailto:bastien.vandelft@epfl.ch), [Anne-Val√©rie Preto](mailto:anne-valerie.preto@epfl.ch)\n",
    "\n",
    "Based on the work of [David Mizrahi](mailto:david.mizrahi@epfl.ch), [Tom Winandy](mailto:tom.winandy@epfl.ch) and [Luc Reveyron](mailto:luc.reveyron@epfl.ch), [Brian Sifringer](mailto:brian.sifringer@epfl.ch)\n",
    "\n",
    "<hr style=\"clear:both\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "8329152929474b2c9c50d88c46fd728c",
    "deepnote_cell_type": "markdown",
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "76086f89bfc06def21a14f128e698c97",
     "grade": false,
     "grade_id": "cell-4329bf3862922f2b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Name and SCIPER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "9a6ecf364c7a4a59acd157ff0fa9874d",
    "deepnote_cell_type": "markdown",
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9a8dbdd0a97fb834224efd5e3914e635",
     "grade": true,
     "grade_id": "cell-50ad389fa3b6e851",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "**Name:** YOUR NAME HERE\n",
    "\n",
    "**SCIPER:** YOUR SCIPER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "46c6b3551d304c13ab2c8e975df0a944",
    "deepnote_cell_type": "markdown",
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d6f52733a8ba3666ad9b21c4eaafbfa9",
     "grade": false,
     "grade_id": "cell-a359c02e17be60ff",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Homework info\n",
    "**Released:** Thursday March 28, 2024  \n",
    "**Submission**: Saturday April 13, 2024 (before 11:59PM) on Moodle  \n",
    "**Grade weight:** 10% of the overall grade \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "\n",
    "**Warning:** This is an individual assignment, the exchange of code between students is forbidden.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "573f885cd0d44ad5bfd3876c5cfe1c41",
    "deepnote_cell_type": "markdown",
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1a96cb477576730225e34532193df1d8",
     "grade": false,
     "grade_id": "cell-660a6d28a7409b23",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "This homework is composed of 4 parts:\n",
    "- Part 1: Logistic regression (43 pts)\n",
    "- Part 2: Regularization (15 pts)\n",
    "- Part 3: Cross-validation (20 pts)\n",
    "- Part 4: Evaluation metrics (22 pts)\n",
    "\n",
    "**Total:** 100 pts\n",
    "\n",
    "**Work through this exercise in order, as functions implemented early on can be used for later parts.**\n",
    "\n",
    "**Tips:**\n",
    "- Use the Table of Contents extension to quickly navigate to each part.\n",
    "- In order to ensure that there are no hidden states in your notebook, frequently restart the kernel using: **Kernel -> Restart Kernel and Run all Selected Cells...**\n",
    "\n",
    "Good luck!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "ea5d5f1e8ffc4a368f2d3997b50b467f",
    "deepnote_cell_type": "markdown",
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1a4287e09afa7221b8075b3ef0381cb8",
     "grade": false,
     "grade_id": "cell-29aba77143b75021",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "4b2e763dddab4601a62b5ad3b00c35f7",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "deletable": false,
    "editable": false,
    "execution_millis": 2,
    "execution_start": 1679936022378,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4749c91aff183a6a69f159fd7cf551cf",
     "grade": false,
     "grade_id": "cell-6c4dc1386371cb36",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "source_hash": "c321d9b9"
   },
   "outputs": [],
   "source": [
    "# Function to align all tables to the left (useful for later on)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "dcbe5cd3afb94aaa90b25ff00b22c471",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "deletable": false,
    "editable": false,
    "execution_millis": 6,
    "execution_start": 1679936022379,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "514a51ab056d3901500c397e76667761",
     "grade": false,
     "grade_id": "cell-5c5603eceb000cf6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "source_hash": "b3d16ff7"
   },
   "outputs": [],
   "source": [
    "%%html\n",
    "<style>\n",
    "table {float:left}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "e4551e0ba55848b8a1895cf526d11206",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "deletable": false,
    "execution_millis": 1836,
    "execution_start": 1679936022386,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b2892da85038b0c1fea11c4fc48b188e",
     "grade": false,
     "grade_id": "cell-a00d78f9fc2e17f5",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "source_hash": "28f3762b"
   },
   "outputs": [],
   "source": [
    "from typing import Tuple, List\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import helpers\n",
    "\n",
    "# You do not require more imports, but if you do use any, put them below. Else, remove the ellipsis\n",
    "### START CODE HERE ###\n",
    "\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "7697e1ab3b82429390765d921551d911",
    "deepnote_cell_type": "markdown",
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9d47edc0fda0c8e6a405064fe700118d",
     "grade": false,
     "grade_id": "cell-b9652397ee687d17",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Part 1: Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "2fe8c68260bb4774a14d2e2f3f608d2c",
    "deepnote_cell_type": "markdown",
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f4c47bd8eea953bc4ba2df7b234d7a3e",
     "grade": false,
     "grade_id": "cell-12a759793df81f98",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The first part of this exercise consists of implementing logistic regression for a binary classification problem. In this part, you will use a slightly modified version of the Palmer iris dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "f6d59f6c10be42dc88cd6f613034092d",
    "deepnote_cell_type": "markdown",
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1a7f3b3b31cb0fc1402cf2549d6d5636",
     "grade": false,
     "grade_id": "cell-34a8a71f636f9a09",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 1.1. Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "b769eda22f0e41619840deca19bc3473",
    "deepnote_cell_type": "markdown",
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "25019d479c4409b680db276ad3a25572",
     "grade": false,
     "grade_id": "cell-c025d0311563a63c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Loading the data & normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "f4f2e5f9dd2544e797731aef42eef251",
    "deepnote_cell_type": "markdown",
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "265130e8050366af841cba8ccc19073a",
     "grade": false,
     "grade_id": "cell-99def0ebc4b0507a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "First, you'll work on a relatively simple dataset: a modified version of the Iris dataset with only two features (Petal length and Petal with), and two species (Iris-versicolor and Iris-virginica). Using `helpers.preprocess_data()` (check helpers.py for more info), we'll obtain a training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "4adc2add3fb44302a0db7244aa637af4",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "deletable": false,
    "editable": false,
    "execution_millis": 52,
    "execution_start": 1679936024264,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "19bc0a001682fc4048d5d6b6e63729bb",
     "grade": false,
     "grade_id": "cell-a8f5c512dfcf3c81",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "source_hash": "3430ff29"
   },
   "outputs": [],
   "source": [
    "iris = pd.read_csv('data/iris_m.csv')\n",
    "iris.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "660921aa132248029007d3e9973678ff",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "deletable": false,
    "editable": false,
    "execution_millis": 436,
    "execution_start": 1679936024315,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4ad9356ef141c21d27937d807ec2bfec",
     "grade": false,
     "grade_id": "cell-31278307d26e8851",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "source_hash": "49b6fca1"
   },
   "outputs": [],
   "source": [
    "ax1 = iris.loc[iris[\"Species\"]==\"Iris-versicolor\"].plot(kind=\"scatter\",x = 'PetalLengthCm', y = 'PetalWidthCm', color=\"DarkGreen\", label=\"iris versicolor\")\n",
    "ax2 =iris.loc[iris[\"Species\"]==\"Iris-virginica\"].plot(kind=\"scatter\",x = 'PetalLengthCm', y = 'PetalWidthCm', color=\"DarkRed\", label=\"iris virginica\", ax=ax1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "67713111675e4314aaa4ca850ed6aa72",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "deletable": false,
    "editable": false,
    "execution_millis": 3,
    "execution_start": 1679936024792,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9dbfd06d94228903159718e723932267",
     "grade": false,
     "grade_id": "cell-6fb3a94edb2c852a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "source_hash": "a5d49a1a"
   },
   "outputs": [],
   "source": [
    "X_train_iris, y_train_iris, X_test_iris, y_test_iris, feature_names, label_map = helpers.preprocess_data(iris, label=\"Species\", train_size=0.70, seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "a4502ead3daf4cecbea942d4ed7f6033",
    "deepnote_cell_type": "markdown",
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "550b56bbc12220cc69fb750e11182d56",
     "grade": false,
     "grade_id": "cell-6f0f94ffbaf6e12b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Next, you need to normalize this data with the mean and standard deviation from the training set and keep the original variable name. For example, the normalized `X_train_iris` should be called `X_train_iris`.\n",
    "\n",
    "**Task: Normalize the training and test data using `helpers.normalize()`**\n",
    "\n",
    "**Note:** Remember that you should not use any of the knowledge you get from the test data when implementing a model. This includes the normalization step, where you should use the mean and standard deviation of the **training set** to normalize both the **training** and **test** set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "cfb65b8a46864e8da7c66b00b1b1b30e",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "deletable": false,
    "execution_millis": 2,
    "execution_start": 1679936024793,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7b533b244246f49a9869c73f74c9af25",
     "grade": false,
     "grade_id": "cell-af2ef90ad754aa43",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "source_hash": "d850d2a0"
   },
   "outputs": [],
   "source": [
    "# Normalize features of the training and test set using the mean and std of the training set features\n",
    "### START CODE HERE ###\n",
    "mean = ...\n",
    "std = ...\n",
    "\n",
    "X_train_iris = ...\n",
    "X_test_iris = ...\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "308ef6df51c44bb29e06e059c03fd9a9",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "deletable": false,
    "editable": false,
    "execution_millis": 5,
    "execution_start": 1679936024794,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "835b8949bd6987180a3928b110f4f2d2",
     "grade": true,
     "grade_id": "cell-ce6c80d103e4b490",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "source_hash": "2d5741eb"
   },
   "outputs": [],
   "source": [
    "# These assertions check that your normalization didn't go completely wrong. \n",
    "# Passing these assertions does not mean you will automatically get all points for that question\n",
    "# We may use other tests to check the correctness of your implementation\n",
    "assert np.allclose(X_train_iris.mean(axis=0), 0)\n",
    "assert np.allclose(X_train_iris.std(axis=0), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "46fa302d3958464d9f6b9b0d3ac53ff7",
    "deepnote_cell_type": "markdown",
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0d0827c64519e77ceb6e7f4e0fb56837",
     "grade": false,
     "grade_id": "cell-b0eb41b993dd1c5d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let's preview the arrays for the training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "8861c48882764c5abe8e1d37ebaec3a5",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "deletable": false,
    "editable": false,
    "execution_millis": 9,
    "execution_start": 1679936024803,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c243ced0b08e5cfc16c9bdf090fa273f",
     "grade": false,
     "grade_id": "cell-16c4bbabcc5aa885",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "source_hash": "5385c079"
   },
   "outputs": [],
   "source": [
    "# Preview of X_train_iris and y_train_iris (separation of the features and the labels)\n",
    "print('Training set features:')\n",
    "print(f'X_train_iris: \\n {X_train_iris[:3]}')\n",
    "\n",
    "print('\\nTraining set labels:')\n",
    "print(f'y_train_iris: \\n {y_train_iris[:3]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "8cdc29aa9adc4265b334661e9d931eed",
    "deepnote_cell_type": "markdown",
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fd15e406e0c09786c7553adcf5408b8a",
     "grade": false,
     "grade_id": "cell-d2e1475a6abc6da5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Expected output:**\n",
    "\n",
    "|                 |                                                  |\n",
    "|-----------------|--------------------------------------------------|\n",
    "| **X_train_iris** | [[0.10228376 0.42612767] <br> [0.34499099 1.9052485 ]<br> [0.22363737 0.91916795]]\n",
    " |\n",
    "| **y_train_iris**  | [[0 1] [0 1] [0 1]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "0728fa4e312040d3b7977f3de41267b2",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "deletable": false,
    "editable": false,
    "execution_millis": 6,
    "execution_start": 1679936024811,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "76fc9002050301f505648902c3628597",
     "grade": false,
     "grade_id": "cell-64ffa9b4b7080e3b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "source_hash": "ae803314"
   },
   "outputs": [],
   "source": [
    "# Preview of X_train_iris and y_train_iris (separation of the features and the labels)\n",
    "print('Test set features:')\n",
    "print(f'X_test_iris: \\n {X_test_iris[:3]}')\n",
    "\n",
    "print('\\nTraining set labels:')\n",
    "print(f'y_test_iris: \\n {y_test_iris[:3]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "b541eb0cba1049d0ac11a1b5972dce9f",
    "deepnote_cell_type": "markdown",
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "70ac4631e047544fdc61a2fcb31ad268",
     "grade": false,
     "grade_id": "cell-d7c48102306b8ddc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Expected output:**\n",
    "\n",
    "|                 |                                                  |\n",
    "|-----------------|--------------------------------------------------|\n",
    "| **X_test_iris** |  [[ 1.92258796  0.91916795]<br>[-1.83937406 -1.54603344]<br>[ 1.07311267  1.16568808]] |\n",
    "| **y_test_iris**  | [[0 1] [1 0] [0 1]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "3bf7f00e91724a52b662059a466420f7",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "deletable": false,
    "editable": false,
    "execution_millis": 48,
    "execution_start": 1679936024818,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d9d6a0614b084326f19cfb18b89e65f8",
     "grade": false,
     "grade_id": "cell-e37b6514fc8d6bb3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "source_hash": "1863d4a2"
   },
   "outputs": [],
   "source": [
    "# Show shapes\n",
    "print('Training set shape:')\n",
    "print(f'X: {X_train_iris.shape}, y: {y_train_iris.shape}')\n",
    "\n",
    "print('\\nTest set shape:')\n",
    "print(f'X: {X_test_iris.shape}, y: {y_test_iris.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "1ebfbee1b4fa4007a11404dbb8ff2091",
    "deepnote_cell_type": "markdown",
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e063343d68dd72367ce3ab6e633ec119",
     "grade": false,
     "grade_id": "cell-baa0bd8769a75b18",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Notation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "f8872933cfb2457e99661b2a146f66fe",
    "deepnote_cell_type": "markdown",
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b49fb77f5f042ba102cc8caf33852dc2",
     "grade": false,
     "grade_id": "cell-81df8f0141a0a158",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now that we have pre-processed our dataset, here's how it looks:\n",
    "\n",
    "- features: $\\boldsymbol{X} \\in \\mathbb{R}^{N \\times D}$, $\\forall \\ \\boldsymbol{x}_j \\in \\boldsymbol{X}: \\boldsymbol{x}_j \\in \\mathbb{R}^{D}$\n",
    "- labels: $\\boldsymbol{y} \\in \\mathbb{R}^{N \\times C }$, $\\forall \\ y_j^{(i)} \\in \\boldsymbol{y}_j: y_j^{(i)} \\in \\{0, 1\\}$ \n",
    "  \n",
    " where $N$ is the number of examples in our dataset, $D$ is the number of features per example and $C$ is the number of classes.   \n",
    " \n",
    "\n",
    "For the weights, we have:\n",
    " \n",
    " \n",
    " - weights: $\\mathbf{W} \\in \\mathbb{R}^{D \\times C}$ \n",
    " - bias: $\\boldsymbol{b} \\in \\mathbb{R}^{C \\times 1}$\n",
    "\n",
    " Note that the labels $\\boldsymbol{y}$ are written as a matrix of 0s and 1s. Each row represents an entry, and has a single 1 to represent the ground truth class, while all others are zero. This is called one-hot encoding and you can learn more here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "bf2f06f69ae049ca9ed5a7e24fd026cd",
    "deepnote_cell_type": "markdown",
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1c498e14e78851fcb911c121df1dbddd",
     "grade": false,
     "grade_id": "cell-ef11677a4d9dd097",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    " **Note:**\n",
    " \n",
    " $\\boldsymbol{X}$ is called the design matrix, where $\\boldsymbol{X}_{j, :}$ denotes $\\boldsymbol{x}_j$.  \n",
    " Note that a single example $\\boldsymbol{x}_j$ is a column vector of shape $(D \\times 1)$, while the design matrix $\\boldsymbol{X}$ is of shape $(N \\times D)$, where each row represents an example and each column represents a feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "1e4e50159ae144ac84e83d89b0b72324",
    "deepnote_cell_type": "markdown",
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1bca5f6c87cf557ef97fd5fde67f80ff",
     "grade": false,
     "grade_id": "cell-1b220c79317ae0db",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 1.2. Diving into logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "b164b2f2a8ca40d3ac6a5b5d06af91c6",
    "deepnote_cell_type": "markdown",
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7847a144d1a7b54dd438ff7283cd7f50",
     "grade": false,
     "grade_id": "cell-650742405fb1f5ee",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In this section, you are going to implement the different functions needed for logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "105945c32a08437d970bee077048dec4",
    "deepnote_cell_type": "markdown",
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a7b2f503c5ee2e9b1ad18384afa8aad6",
     "grade": false,
     "grade_id": "cell-82488d3bf84a193e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "71efb224b2a04fa198551f9f534abb6e",
    "deepnote_cell_type": "markdown",
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "82e3341a5dc918cac0ef57a7ca4242d2",
     "grade": false,
     "grade_id": "cell-b99bf8ea309547ce",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "A key element of multi-class logistic regression is the softmax function. This function takes any real-valued vector and outputs a same-sized vector with values in [0, 1], and that sum to 1. The softmax function is defined as:\n",
    "$$\\sigma_i(\\boldsymbol{z})= \\frac {e^{z_i}}{\\sum_je^{z_j}}= \\frac{1}{1+\\sum_{j\\neq i} e^{zj-zi}}$$\n",
    "\n",
    "Here's an example plot of the softmax in 2D, over values of 1 dimension given all others are equal to 0:\n",
    "\n",
    "<img src=\"images/sigmoid.png\" alt=\"Sigmoid\" style=\"width: 450px\"/>\n",
    "\n",
    "**Task: Implement `softmax()`**  \n",
    "\n",
    "**Hint:** Use `np.exp(x)` to take the exponential of a number. You can find the documentation for this method [here](https://numpy.org/doc/stable/reference/generated/numpy.exp.html).\n",
    "**Hint 2:** `keepdims=` argument, or the function `np.expand_dims()` may help you in certain broadcasting error cases\n",
    "**Note:** Usually, an added term is used in the code of softmax for numerical stability purposes, it will not be required for this homework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "c6aa4feb92f64cad8bf34aa91c59c5f2",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "deletable": false,
    "execution_millis": 3,
    "execution_start": 1679936024890,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "df7a21ea2b5fe19ea4a73261769d46f7",
     "grade": false,
     "grade_id": "cell-e999e5baf8517826",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "source_hash": "96a77e5a"
   },
   "outputs": [],
   "source": [
    "def softmax(z: np.ndarray) -> np.ndarray:\n",
    "    \"\"\" Softmax function\n",
    "    \n",
    "    Args:\n",
    "        z: Input data of shape (N, C)\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: Softmax of z of shape (N, C), where each value is in [0, 1] and their sum is 1.\n",
    "    \"\"\"\n",
    "    ### START CODE HERE ###\n",
    "    s = ...\n",
    "    ### END CODE HERE ###\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "38eb56d0175a4b4185e3b5d9e4970fad",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "deletable": false,
    "editable": false,
    "execution_millis": 2,
    "execution_start": 1679936024891,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a2c65b3a7c901c70aaa42db905ff45aa",
     "grade": true,
     "grade_id": "cell-a4d76e12a5ac0331",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "source_hash": "1139f57d"
   },
   "outputs": [],
   "source": [
    "# Verify implementation\n",
    "a = softmax(np.array([[3,0], [0.5,0], [-1,0]]))\n",
    "print(f'a: {np.round(a, 4)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "e1fa0b737f564c0686764efd6dd650c1",
    "deepnote_cell_type": "markdown",
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2a4efac791c73350f07601bb3732337f",
     "grade": false,
     "grade_id": "cell-fa1fcbd71f17c4d2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Expected output:**\n",
    "\n",
    "|   |                                                  |\n",
    "|---|--------------------------------------------------|\n",
    "| **a** | [[0.9526 0.0474] [0.6225 0.3775] [0.2689 0.7311]] |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "34eb3748775c40ffbe0f266c011b85dd",
    "deepnote_cell_type": "markdown",
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "12b23a0026f87e84bdbe1f3f41bea935",
     "grade": false,
     "grade_id": "cell-53f299fe5ba02f43",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Logistic output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "a3361817cc684832bfd69224c7e79c0f",
    "deepnote_cell_type": "markdown",
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d7124827a44882dbc1170a927bb40380",
     "grade": false,
     "grade_id": "cell-49f5ed2ec12d410d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now that you have a function that computes the softmax function, you can implement a function that gives the logistic output. As a reminder, this function outputs the estimated probabilities of an input $\\boldsymbol{x}_j$ for which they belong to the classes in $$\\boldsymbol{y}_j$$ , that is: $ \\hat{y}_j^{(i)} = p(y^{(i)}_j = 1  | \\boldsymbol{x}_j)$.  \n",
    "The logistic output (of a single example and a single class) is defined as:\n",
    "$$\\hat{y}^{(i)}_j = \\sigma_i(\\mathbf{w}^{T} \\boldsymbol{x}_j + \\boldsymbol{b}) = \\frac{e^{-(\\mathbf{w}^{(i)T}  \\boldsymbol{x}_j + b^{(i)})}}{\\sum_k^C e^{-(\\mathbf{w}^{(k)T}  \\boldsymbol{x}_j + b^{(k)})}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "11068b6b92e943d098c94d6392959f33",
    "deepnote_cell_type": "markdown",
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "42fd7f589f1828d5ff9fd6d8131e0e8d",
     "grade": false,
     "grade_id": "cell-a8ccffd1eb9e3fb0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "For all examples, the output is defined as: \n",
    "$$\\mathbf{\\hat{y}} = \\sigma(\\mathbf{X} \\mathbf{W} + \\mathbf{b})$$\n",
    "\n",
    "To simplify the notation of the problem, you have seen in previous exercise sessions that we can add the intercepts $\\boldsymbol{b}$ into the weight matrix $\\mathbf{w}$ by adding a vector $\\boldsymbol{1}$ into the data features of $\\mathbf{X}$.\n",
    "\n",
    "Mathematically, this creates a new set $\\mathbf{X'}$ of size $(N\\times D')$ where $D'= D+1$, and the weight matrix is also now $\\mathbf{W'}$ of size $(D'\\times C)$. For simplicity reasons, we will continue to write them as $\\mathbf{X}$, $\\mathbf{W}$ and $D$ after their change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "67f828ce6fe74bb38c9e2b19dab4ff3b",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "deletable": false,
    "editable": false,
    "execution_millis": 1,
    "execution_start": 1679936024892,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3e41f3f7da0420e0e83c8587027a50cf",
     "grade": false,
     "grade_id": "cell-690cbd17ce7048b2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "source_hash": "5c7c8324"
   },
   "outputs": [],
   "source": [
    "# Let's add a 1 vector to all our X data:\n",
    "X_train_iris = helpers.insert_offset(X_train_iris)\n",
    "X_test_iris = helpers.insert_offset(X_test_iris)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "804fa345b1584482b8e249283902a79f",
    "deepnote_cell_type": "markdown",
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2b4ddc29bfb6679dc7e68dd7379f71ce",
     "grade": false,
     "grade_id": "cell-3374a361666852bd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Once this is done, we can simply write the logistic ouput as: \n",
    "$$\\mathbf{\\hat{y}} = \\sigma(\\mathbf{X} \\mathbf{W})$$\n",
    "\n",
    "**Task: Implement `logistic_output()`**\n",
    "\n",
    "**Hint:** To vectorize this operation , use `np.matmul(a, b)` (or equivalently `a @ b`) for matrix multiplication.  \n",
    "You can find the documentation for this method [here](https://numpy.org/doc/stable/reference/generated/numpy.matmul.html).\n",
    "\n",
    "**Hint:** Remember that you have already coded the sigmoid function. This might help you in the logistic output implementation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "f3afa75739964594b30171fc6b6960e9",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "deletable": false,
    "execution_millis": 2,
    "execution_start": 1679936024896,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "babeac30706e54023ba6a4a40d7e0e6e",
     "grade": false,
     "grade_id": "cell-6caec77f1b106e66",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "source_hash": "bb55b5e1"
   },
   "outputs": [],
   "source": [
    "def logistic_output(X: np.ndarray, w: np.ndarray) -> np.ndarray:\n",
    "    \"\"\" Output of logistic regression\n",
    "    \n",
    "    Args:\n",
    "        X: Dataset of shape (N, D)\n",
    "        w: Weights of multi-class logistic regression model of shape (D, C)\n",
    "    Returns:\n",
    "        y_hat (np.ndarray): Output of multi-class logistic regression of shape (N, C)\n",
    "    \"\"\"\n",
    "    ### START CODE HERE ###\n",
    "    y_hat = ...\n",
    "    ### END CODE HERE ###\n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "4109fa8b8e5a44c4aaa14cdc57c81b58",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "deletable": false,
    "editable": false,
    "execution_millis": 4,
    "execution_start": 1679936024904,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e53a71ababcde26111d8ab84597981bb",
     "grade": true,
     "grade_id": "cell-b981de04e23093f5",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "source_hash": "c3e2e418"
   },
   "outputs": [],
   "source": [
    "# Verify implementation\n",
    "X = X_train_iris[:3]\n",
    "w = np.array([[0,1],[0,1],[0,1]])\n",
    "y_hat = logistic_output(X, w)\n",
    "print(f'y_hat: {np.round(y_hat, decimals=4)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "aa64c4ac9d33409989becdce045ebdf7",
    "deepnote_cell_type": "markdown",
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fc6f8eaf015942d24defbb5b08f1bf6d",
     "grade": false,
     "grade_id": "cell-80ab5e0354ce2714",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Expected output:**\n",
    "\n",
    "|                 |                                                  |\n",
    "|-----------------|--------------------------------------------------|\n",
    "| **y_hat** |  [[0.1782 0.8218]<br>[0.0373 0.9627]<br>[0.105  0.895 ]]|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "ff2b3bdd25894d6a85527508d1b21d21",
    "deepnote_cell_type": "markdown",
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5789b7b0cb19488e4ad0dbb098ef059e",
     "grade": false,
     "grade_id": "cell-67547d64e2468fa6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Categorical Cross-Entropy Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "606403a5775b4df897a887c0f9c8ce22",
    "deepnote_cell_type": "markdown",
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "54ab0a906f318162f81e9a2cbfd9e6dc",
     "grade": false,
     "grade_id": "cell-0ab39c541f3ffca8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In order to train your model, you also need a loss function, which penalizes outputs based on how far off they are from the ground-truth. Here, we'll use the logistic loss / categorical cross-entropy loss.\n",
    "\n",
    "It is defined for all entries j as:\n",
    "$$\\text{CCE}(\\mathbf{X}, \\boldsymbol{y}, \\mathbf{W}) = - \\frac{1}{N}\\sum^{N}_{j=1}\\sum^{C}_i {y}_j^{(i)} \\log(\\hat{y}_j^{(i)}) $$ where $\\log(x)$ refers to the natural logarithm of $x$.\n",
    "\n",
    "**Task: Implement `cce_loss()`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "908f95c7c48b415dbcb738a4318ced43",
    "deepnote_cell_type": "markdown",
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d90f356070ff267317e1cb4ef47530e9",
     "grade": false,
     "grade_id": "cell-ff9acc1f454da6de",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Hint:** Use `np.log(x)` to take the natural logarithm of x. You can find the documentation for this method [here](https://numpy.org/doc/stable/reference/generated/numpy.log.html).\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "Due to floating-point arithmetic, values very close to 0 will get rounded to 0 (likewise for values very close to 1). \n",
    "\n",
    "However, the natural logarithm log(x) is undefined for values equal to 0. An easy way to fix this is to add a small term $\\epsilon$ to the logarithm. For example, write `np.log(x + epsilon)` instead of `np.log(x)`. \n",
    "    \n",
    "In order to get full points (and avoid problems in later parts of this homework), add  `epsilon` (set to $10^{-9}$) whenever you call `np.log()`.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "fc1c03929e654e3fb63944170de9eaa2",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "deletable": false,
    "execution_millis": 3,
    "execution_start": 1679936024913,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4b6f5a60166bc24ca32977b8eac14b24",
     "grade": false,
     "grade_id": "cell-386968d72cf26d7d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "source_hash": "fc78b948"
   },
   "outputs": [],
   "source": [
    "def cce_loss(X: np.ndarray,  y: np.ndarray, w: np.ndarray) -> float:\n",
    "    \"\"\" Categorical cross-entropy loss function\n",
    "    \n",
    "    Args:\n",
    "        X: Dataset of shape (N, D)\n",
    "        y: Labels of shape (N, C).\n",
    "        w: Weights of logistic regression model of shape (D, C)\n",
    "    \n",
    "    Returns:\n",
    "        float: categorical cross-entropy loss.\n",
    "    \"\"\"\n",
    "    # Add the epsilon term to the np.log() in your implementation (e.g. do np.log(x + epsilon) instead of np.log(x))\n",
    "    # Epsilon is there to avoid log(0)\n",
    "    epsilon = 1e-9\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    loss = ...\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "560538da50b2431a92e649a2181bded2",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "deletable": false,
    "editable": false,
    "execution_millis": 4,
    "execution_start": 1679936024960,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "703893fb02dadbd716b68b63a8c96f84",
     "grade": true,
     "grade_id": "cell-207f9debdf5b4bb3",
     "locked": true,
     "points": 7,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "source_hash": "df27bbbb"
   },
   "outputs": [],
   "source": [
    "# Verify implementation\n",
    "X = X_train_iris[:3]\n",
    "y  = y_train_iris[:3]\n",
    "w = np.array([[0,1],[0,1],[0,1]])\n",
    "\n",
    "loss = cce_loss(X, y, w)\n",
    "print(f'Loss: {loss:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "cda44be55b79420daa0bd036519c199c",
    "deepnote_cell_type": "markdown",
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "08e222431a94bfc056733f97fa7f6534",
     "grade": false,
     "grade_id": "cell-7a9d074931ea7dba",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Expected output:**\n",
    "\n",
    "|                 |                                                  |\n",
    "|-----------------|--------------------------------------------------|\n",
    "| **Loss** | 0.115|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "0b9aee05a3dc40d9a105320ad778c639",
    "deepnote_cell_type": "markdown",
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f88fde9296b0e09210c0fb9d5b38a74f",
     "grade": false,
     "grade_id": "cell-07d0f9b9903a2b75",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Categorical Cross-Entropy Loss Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "725ef7ebbcdb4b4ea3a44bf2f01c00dc",
    "deepnote_cell_type": "markdown",
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a5351c785b4018913ea9114602bfc26a",
     "grade": false,
     "grade_id": "cell-0dabc2f9cc729e75",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "\n",
    "After having computed the loss, you need to update the parameters of the model in order to decrease it. This can be done with gradient descent, and for that, you'll need the gradient of your loss function.\n",
    "\n",
    "We can calculate the gradient of cross entropy loss, by applying chain rule. Please note that we consider softmax as the activation function as mentioned above, and z is the logits (output before softmax). This is for a a single entry, where L stands for a single entry CCE-Loss:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial z_i} = - \\sum^C_{j=1}{\\frac{\\partial ({y}^{(j)} \\log(\\hat{y}^{(j)}))}{\\partial z_i}} =   - \\sum^C_{j=1}{{y}^{(j)} \\frac{\\partial  \\log(\\hat{y}^{(j)})}{\\partial z_i}} = - \\sum^C_{j=1}{{y}^{(j)} \\frac{1}{\\hat{y}^{(j)}} \\frac{\\partial \\hat{y}^{(j)}}{\\partial z_i}} = - \\frac{{y}^{(i)}}{\\hat{y}^{(i)}} \\frac{\\partial \\hat{y}^{(i)}}{\\partial z_i} - \\sum_{i \\not= j}{\\frac{{y}^{(j)}}{\\hat{y}^{(j)}} \\frac{\\partial \\hat{y}^{(j)}}{\\partial z_i}} $$\n",
    "$$= - \\frac{{y}^{(i)}}{\\hat{y}^{(i)}} {\\hat{y}^{(i)}}(1-{\\hat{y}^{(i)}}) - \\sum_{i \\not= j}{ \\frac{{y}^{(j)}}{\\hat{y}^{(j)}}(-\\hat{y}^{(j)}\\hat{y}^{(i)})} = - {y}^{(j)} + {y}^{(i)} \\hat{y}^{(i)} + \\sum_{i \\not= j}{{y}^{(j)} \\hat{y}^{(i)}} = - {y}^{(i)} + \\sum^C_{j=1}{{y}^{(j)} \\hat{y}^{(i)}} $$ \n",
    "\n",
    "$$= - {y}^{(i)} +  \\hat{y}^{(i)} \\sum^C_{j=1}{{y}^{(j)}} = -{y}^{(i)} + \\hat{y}^{(i)}$$\n",
    "\n",
    "\n",
    "Since $\\mathbf{z} = \\mathbf{Xw}$, we can easily derive the gradient with respect to $\\mathbf{w}$\n",
    "\n",
    "A vectorized implementation of this expression over the entire dataset can be derived from the following:  \n",
    "\n",
    "$$\\frac{\\partial \\text{CCE}(\\mathbf{w})}{\\partial \\mathbf{w}} = \\frac{1}{N}\\sum_j^N\\frac{\\partial \\mathbf{z_j}}{\\partial \\mathbf{w}}\\frac{\\partial L}{\\partial \\mathbf{z_j}} = \\frac{1}{N} \\ \\left(\\frac{\\partial \\mathbf{X} \\mathbf{w}}{\\partial \\mathbf{w}}\\right)^T  (\\mathbf{ \\hat{y}} - \\mathbf{{y}})$$\n",
    "\n",
    "**Task: Implement `cce_gradient()`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "6af4c6d0ebc1497d8fb1bcb452b2b737",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "deletable": false,
    "execution_millis": 2,
    "execution_start": 1679936024970,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f5beb065da745cf210bc8b78194ffd2c",
     "grade": false,
     "grade_id": "cell-363cee6ca6251a9e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "source_hash": "fa7d5524"
   },
   "outputs": [],
   "source": [
    "def cce_gradient(X: np.ndarray,  y: np.ndarray, w: np.ndarray) -> Tuple[np.ndarray, float]:\n",
    "    \"\"\" Gradient of the categorical cross-entropy loss\n",
    "    \n",
    "    Args:\n",
    "        X: Dataset of shape (N, D)\n",
    "        y: Labels of shape (N, C)\n",
    "        w: Weights of logistic regression model of shape (D, C)\n",
    "        \n",
    "    Returns:\n",
    "        dw (np.ndarray) gradient of the loss with respect to w of shape (D, C)\n",
    "    \"\"\"\n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    dw = ...\n",
    "    ### END CODE HERE ###\n",
    "    return dw\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "a36e08bceddc4484bc6fb40442ccabc4",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "deletable": false,
    "editable": false,
    "execution_millis": 4,
    "execution_start": 1679936024999,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a3256945c51c968c679473cf0e136f35",
     "grade": true,
     "grade_id": "cell-64f2d5148122e30d",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "source_hash": "66f58cfd"
   },
   "outputs": [],
   "source": [
    "# Verify implementation\n",
    "X = X_train_iris[:3]\n",
    "y  = y_train_iris[:3]\n",
    "w = np.array([[1,0], [-1,0], [0,2]])\n",
    "\n",
    "\n",
    "dw = cce_gradient(X, y, w)\n",
    "print(f'dw: {np.round(dw, decimals=4)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "a160cb90e6d1494582eb6cc45f705c80",
    "deepnote_cell_type": "markdown",
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "57a1346893c0937f4ba4d6d7199ba698",
     "grade": false,
     "grade_id": "cell-5cfb006208bff804",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Expected output:**\n",
    "\n",
    "|    |                                   |\n",
    "|----|-----------------------------------|\n",
    "| **dw** | [[ 0.2697 -0.2697]<br>[ 0.0413 -0.0413]<br>[ 0.1773 -0.1773]]|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "6da16db4ce0a4b35b672a1194d0ae2dd",
    "deepnote_cell_type": "markdown",
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "725196c5857c491c259aaf1bd8983a12",
     "grade": false,
     "grade_id": "cell-f402f06e5d04a8e4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "790800a7183a4089956624ca96395f58",
    "deepnote_cell_type": "markdown",
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "03fa1adfcac9e911be2374293e40d663",
     "grade": false,
     "grade_id": "cell-20d859b7370fc8be",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The output of logistic regression estimates $ \\hat{y}^{(i)} = P(y^{(i)} = 1  | \\boldsymbol{x})$ for every class. The softmax function has the proprety that $\\sum_i\\hat{y}^{(i)} = 1$.\n",
    "\n",
    "Based on this output, you'll want to classify each example in its correct class $c\\in \\{0,...,C-1\\}$. To do so, you'll need to find which index i in $\\hat{y}^{(i)}$ has the highest probability score, and use this index as your class prediction.\n",
    "\n",
    "**Task: Implement `classify()`**\n",
    "\n",
    "**Hint:** Use `np.argmax()`. You can find the documentation for this method [here](https://numpy.org/doc/stable/reference/generated/numpy.argmax.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "a0fac363a01b4e3c98d1e2ec09c2947e",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "deletable": false,
    "execution_millis": 3,
    "execution_start": 1679936025000,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b4cb1235eefcf17eabb4dfc087197d3f",
     "grade": false,
     "grade_id": "cell-5c92d62d33b944f1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "source_hash": "14bca904"
   },
   "outputs": [],
   "source": [
    "def classify(y_hat: np.ndarray) -> np.ndarray:\n",
    "    \"\"\" Classification function for  multi-class logistic regression. \n",
    "    \n",
    "    Args:\n",
    "        y_hat (np.array): Output of logistic regression of shape (N, C).\n",
    "    Returns:\n",
    "        np.array: Label assignments (indices) of data of shape (N, )\n",
    "    \"\"\"\n",
    "    ### START CODE HERE ###\n",
    "    labels_pred = ...\n",
    "    ### END CODE HERE ###\n",
    "    return labels_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "c0ac95d4fb92476285a31d5ffdba70df",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "deletable": false,
    "editable": false,
    "execution_millis": 4,
    "execution_start": 1679936025001,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ab21f01d5b5742f411f8f0618175a199",
     "grade": true,
     "grade_id": "cell-b6f0cd5acea4b630",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "source_hash": "f00b958b"
   },
   "outputs": [],
   "source": [
    "# Example of 2 predictions for C=2 \n",
    "y_hat = np.array([[0.6, 0.4], [0.25, 0.75]])\n",
    "print(classify(y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "56f9499145174de99496d7ddfa263fd1",
    "deepnote_cell_type": "markdown",
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "01a7f29ad34d4642df5c4f088558aa36",
     "grade": false,
     "grade_id": "cell-54ef487260985115",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Expected output:** [0 1] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "610801cbf6834cb3bb471c996393781f",
    "deepnote_cell_type": "text-cell-h3",
    "deletable": false,
    "editable": false,
    "formattedRanges": [],
    "is_collapsed": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8e3abbaa743da61d618a8e1c3ffa9ddd",
     "grade": false,
     "grade_id": "cell-942d57db088426ad",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "### Converting One-Hot labels to Indices "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "e399a297-a241-46a9-89aa-61b6df377afc",
    "deepnote_cell_type": "text-cell-p",
    "deletable": false,
    "editable": false,
    "formattedRanges": [],
    "is_collapsed": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1568271114fb0b4d9eb6b1b39c56b586",
     "grade": false,
     "grade_id": "cell-3e9e6f29c4f48072",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "You may have realized that we are now able to classify a prediction by assigning it to a class number, but that are ground truths are still one-hot encoded. For many of the upcoming functions, it will be easier to have ground truths as class indices instead. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "04eb0d900c5c4c1b9e77016997be777b",
    "deepnote_cell_type": "markdown",
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4dd669ff8024a04edad26eccfbdaefd6",
     "grade": false,
     "grade_id": "cell-f7a4a3e4037d469a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "Mathematically, we currently have $\\boldsymbol{y} \\in \\mathbb{R}^{N \\times C}$, $\\forall \\ y^{(i)} \\in \\boldsymbol{y}: y^{(i)} \\in \\{0, 1\\}$. \n",
    "Our function will map the labels to $\\boldsymbol{y} \\in \\mathbb{R}^{N}$, $\\forall \\ y^{(i)} \\in \\boldsymbol{y}: y^{(i)} \\in \\{0, 1, ..., C-1\\}$, where $y^{(i)}$ takes the value of the index in which it was one-hot encoded. \n",
    "\n",
    "**Hint:** It is exactly the same as the previous function called classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "44283be8b4d04990a3cc10baf2a833b5",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "deletable": false,
    "execution_millis": 3,
    "execution_start": 1679936025006,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "326e9c5edc031dbc79e7e260762d0f4c",
     "grade": false,
     "grade_id": "cell-ac774f578c107f7b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "source_hash": "351fde60",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def convert_onehot_to_class(y: np.ndarray) -> np.ndarray:\n",
    "    \"\"\" Turn a one-hot label matrix into an class-based label array. \n",
    "    \n",
    "    Args:\n",
    "        y (np.array): Ground Truth labels of shape (N, C).\n",
    "    Returns:\n",
    "        np.array: Label assignments (indices) of data of shape (N, )\n",
    "    \"\"\"\n",
    "    ### START CODE HERE ###\n",
    "    y_class_labels = ...\n",
    "    ### END CODE HERE ###\n",
    "    return y_class_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "2967f78b5a6d49b0b87b9379dbfb9f18",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "deletable": false,
    "editable": false,
    "execution_millis": 5,
    "execution_start": 1679936025009,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "021046d4fe07c0fef1ba0e1394d83a49",
     "grade": true,
     "grade_id": "cell-24a8156e20033b4a",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "source_hash": "8b7e5e68"
   },
   "outputs": [],
   "source": [
    "# Example of ground truth labels: \n",
    "y = np.array([[0,1,0], [0,0,1]])\n",
    "print(convert_onehot_to_class(y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "0b471bc2b89e4bbc92298aba0ae3e26f",
    "deepnote_cell_type": "markdown",
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "39484951774f887d62f3d536bea78073",
     "grade": false,
     "grade_id": "cell-4f6df3255efe86c3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Expected output:** [1 2] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "b349dd6632fa495dafd6cd57cb1a68ff",
    "deepnote_cell_type": "markdown",
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "115dfd36116ba1fcb12659231f301b7c",
     "grade": false,
     "grade_id": "cell-f3237226b328f5df",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "43b4e24b06194eeea21900df748cf827",
    "deepnote_cell_type": "markdown",
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4e70c533c15381128077370f89be0cc1",
     "grade": false,
     "grade_id": "cell-dacad3b4c6341c69",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "To measure how well your model is doing, we'll first consider the accuracy as our metric. It corresponds to the fraction of predictions our model got right. Note that we inputs are index based ground truths and classified predictions.\n",
    "\n",
    "$$\\text{Accuracy} = \\frac{\\text{Number of correct predictions}}{\\text{Total number of predictions}}$$\n",
    "\n",
    "\n",
    "\n",
    "**Task: Implement `accuracy()`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "96b93b7a53a14538ae7c5c6216acf91e",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "deletable": false,
    "execution_millis": 3,
    "execution_start": 1679936025085,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "57807091b1e073f83856748b45e9d073",
     "grade": false,
     "grade_id": "cell-9da5f76d65e389ce",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "source_hash": "111ef2d2"
   },
   "outputs": [],
   "source": [
    "def accuracy(labels_class_gt: np.ndarray, labels_pred: np.ndarray) -> float:\n",
    "    \"\"\"Computes the accuracy.\n",
    "\n",
    "    Args:\n",
    "        labels_class_gt: labels as indices (ground-truth) of shape (M, ).\n",
    "        labels_pred: Predicted labels as indices of shape (M, ).\n",
    "\n",
    "    Returns:\n",
    "        float: Accuracy, in range [0, 1].\n",
    "    \"\"\"\n",
    "    ### START CODE HERE ###\n",
    "    return ...\n",
    "    ### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "ed82fb5d70d34c7aa0f441178e6e8181",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "deletable": false,
    "editable": false,
    "execution_millis": 2,
    "execution_start": 1679936025086,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "66c8b448450e7b2aec8a51ba3e5f49fb",
     "grade": true,
     "grade_id": "cell-bceb4550413f7dd6",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "source_hash": "47cbd8bf"
   },
   "outputs": [],
   "source": [
    "# Check that output is in [0, 1] (ensures that output is not a percentage)\n",
    "assert 0.0 <= accuracy(np.array([1, 0]), np.array([1, 1])) <= 1.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**Discuss** the drawbacks of using accuracy as the sole evaluation metric for classification tasks. What are some scenarios where accuracy might not provide a complete picture of model performance? Provide examples to support your explanation.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> WRITE YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "bbcd87015e16419f9632255dfa3d565d",
    "deepnote_cell_type": "markdown",
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3d687856e5425c58373a0fcf086c0f2e",
     "grade": false,
     "grade_id": "cell-6af2a738f8f3701f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 1.3. Training\n",
    "You will now implement the training process by putting together all the functions you have implemented so far.\n",
    "\n",
    "Here are the different steps of the training process:\n",
    "- Compute the output of logistic regression (`y_hat`)\n",
    "- Compute the loss of the model (`loss`)\n",
    "- Compute the derivates w.r.t to the weights `dw`\n",
    "- Update `w`\n",
    "\n",
    "Recall that a gradient step is: \n",
    "$$ \\mathbf{w}  := \\mathbf{w} - \\alpha \\frac{\\partial J}{\\partial \\mathbf{w}} $$\n",
    "\n",
    "where $J$ is the loss function and $\\alpha$ is the learning rate.\n",
    "\n",
    "**Task: Complete the following `train_logistic_regression()` function.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "78847b563e62467bb6463e47bad4df83",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "deletable": false,
    "execution_millis": 12,
    "execution_start": 1679936025087,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f5dcdf16ce77fe6ebb9b54801e36cb0b",
     "grade": false,
     "grade_id": "cell-740a96ad4a0a6142",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "source_hash": "794d915"
   },
   "outputs": [],
   "source": [
    "def train_logistic_regression(X: np.ndarray, \n",
    "                              y: np.ndarray, \n",
    "                              max_iters: int = 101, \n",
    "                              lr: float = 0.5, \n",
    "                              loss_freq: int = 0) -> Tuple[np.ndarray, float, dict]:\n",
    "    \"\"\" Training function for binary class logistic regression using gradient descent\n",
    "    \n",
    "    Args:\n",
    "        X: Dataset of shape (N, D).\n",
    "        y: Labels of shape (N, C).\n",
    "        max_iters: Maximum number of iterations. Default : 100\n",
    "        lr: The learning rate of  the gradient step. Default : 1\n",
    "        loss_freq : Prints the loss every `loss_freq` iterations. Default : 0\n",
    "        \n",
    "    Returns:\n",
    "        w: weights of shape (D, C)\n",
    "        viz_d: dict used for visualizations\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize weights\n",
    "    np.random.seed(0)\n",
    "    y_class = convert_onehot_to_class(y)\n",
    "    w = np.random.normal(0, 1, size=(X.shape[1],y.shape[1]))\n",
    "    \n",
    "    # Initialize dict with lists to keep track of loss, accuracy, weight and bias evolution\n",
    "    logger = {'loss': [], \n",
    "             'acc': [], \n",
    "             'w': []\n",
    "            }\n",
    "    \n",
    "    \n",
    "    for i in range(max_iters):\n",
    "        # Compute loss, dw, db and update w and b \n",
    "        ### START CODE HERE ###\n",
    "        #y_hat= logistic_output(X,w) \n",
    "        loss = ...\n",
    "        dw = ...\n",
    "        \n",
    "        w = ...\n",
    "        b = ...\n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Keep track of parameter, loss and accuracy values for each iteration\n",
    "        logger['w'].append(w)\n",
    "        logger['loss'].append(loss)\n",
    "        y_hat = logistic_output(X, w)\n",
    "        logger['acc'].append(accuracy(y_class, classify(y_hat)))\n",
    "        \n",
    "        if (loss_freq !=0) and i % loss_freq == 0:\n",
    "            print(f'Loss at iter {i}: {loss:.5f}')\n",
    "        \n",
    "    if (loss_freq != 0):\n",
    "        print('\\nFinal loss: {:.5f}'.format(logger['loss'][-1]))\n",
    "        \n",
    "    return w, logger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "611c6d6003524772b701a208f79f7cf3",
    "deepnote_cell_type": "markdown",
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "823dde43fad469b1af88b4b7f3864cc9",
     "grade": false,
     "grade_id": "cell-426d1196f5585a43",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now, run the following cell with the default hyperparameters (max_iters=101, lr=0.5) to train your model parameters (`w`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "079033650dfd4d6880bd4bae758163e3",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "deletable": false,
    "editable": false,
    "execution_millis": 70,
    "execution_start": 1679936025098,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5c4e25772ed9e2639404167a9eeb88e2",
     "grade": true,
     "grade_id": "cell-afbf4cf924cb68a4",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "source_hash": "e06dea34"
   },
   "outputs": [],
   "source": [
    "w, logger = train_logistic_regression(X_train_iris, y_train_iris, max_iters=101, lr=0.5, loss_freq=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "7d3e8364445d40cdb386d1b1507e1e69",
    "deepnote_cell_type": "markdown",
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "56422cfdc47c9331e729364731863d3c",
     "grade": false,
     "grade_id": "cell-d15cf2a6e36eb5b7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Expected output:**\n",
    "\n",
    "|            |         |\n",
    "|------------|---------|\n",
    "| **Final loss**  | 0.10853 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "aaa30fc93b514c768731211ada709f2e",
    "deepnote_cell_type": "markdown",
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7d7d196eadce0dac03d146014fcde93c",
     "grade": false,
     "grade_id": "cell-e09d611c113b07e0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 1.4. Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "a56c0ae9dcbf4f7ebc03edb6a9d75bb2",
    "deepnote_cell_type": "markdown",
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f22822c63691eeaf68aa2a051ec899ab",
     "grade": false,
     "grade_id": "cell-0f28f624609e8906",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Model accuracy\n",
    "Great, your model is trained! But how well does it perform? To find out, run the following cells to get the accuracy of your trained model on the train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "84b146b8bab04b15a203040bdba62a71",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "deletable": false,
    "editable": false,
    "execution_millis": 3,
    "execution_start": 1679936025165,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9005778dc2add8e746879622ca80d64b",
     "grade": false,
     "grade_id": "cell-227db156a18afcd3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "source_hash": "6bd90a96"
   },
   "outputs": [],
   "source": [
    "# Train acc\n",
    "y_hat = logistic_output(X_train_iris, w)\n",
    "y_class = convert_onehot_to_class(y_train_iris)\n",
    "acc = accuracy(y_class, classify(y_hat))\n",
    "print(f'Train accuracy: {100 * acc:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "2464c790f41642159328c2dc7dc14249",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "deletable": false,
    "editable": false,
    "execution_millis": 3,
    "execution_start": 1679936025212,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "639846d39bc5d121d437d3cb12def3f3",
     "grade": false,
     "grade_id": "cell-837eef1af1393c46",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "source_hash": "dc1a9e76"
   },
   "outputs": [],
   "source": [
    "# Test acc\n",
    "y_hat = logistic_output(X_test_iris, w)\n",
    "y_class = convert_onehot_to_class(y_test_iris)\n",
    "acc = accuracy(y_class, classify(y_hat))\n",
    "print(f'Test accuracy: {100*acc:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "6311679c607d413e912e797c4a5a7ffd",
    "deepnote_cell_type": "markdown",
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8d930a5c6bfed464eaf027b425d56431",
     "grade": false,
     "grade_id": "cell-ab4bfc1e03c3c0c5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Expected output:**\n",
    "\n",
    "|            |         |\n",
    "|------------|---------|\n",
    "| **Train accuracy**  | 94.29% |\n",
    "| **Test accuracy**  | 93.33% |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "0f3cba2013cd4c99812f86b3cc63f28d",
    "deepnote_cell_type": "markdown",
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "036402727a3902de26bbf42e27a58921",
     "grade": false,
     "grade_id": "cell-a42ae83845139239",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Visualization of the training\n",
    "Let's observe how the training went. The first graph plots the evolution of the loss during the training while the second graph plots the evolution of the accuracy during the training. If everything was implemented correctly, you'll notice that as training goes on, the loss decreases and the training accuracy tends to increase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "d6b8a56d13f84b35b1926b4eae1b7623",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "deletable": false,
    "editable": false,
    "execution_millis": 313,
    "execution_start": 1679936025213,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4a8f3d7832c1e29f0eaebb143fd88719",
     "grade": false,
     "grade_id": "cell-b0fa9a61c7139be5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "source_hash": "4d872178"
   },
   "outputs": [],
   "source": [
    "# Plot the evolution of loss during training\n",
    "def plot_loss(loss_list):\n",
    "    fig = plt.figure(figsize=(6, 6))\n",
    "    ax = fig.add_subplot(111)\n",
    "    step = np.arange(1, len(loss_list)+1)\n",
    "    plt.plot(step, loss_list)\n",
    "    plt.title('Evolution of the loss during the training')\n",
    "    plt.xlabel('iteration')\n",
    "    plt.ylabel('Training loss')\n",
    "    plt.show()\n",
    "\n",
    "plot_loss(logger[\"loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "ecca8a70bd484d76bcdd33aa0d8089e5",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "deletable": false,
    "editable": false,
    "execution_millis": 493,
    "execution_start": 1679936025530,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a7224a3e8de89937ab9c67a4b78eed7b",
     "grade": false,
     "grade_id": "cell-da6e6f9327a2f10f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "source_hash": "f03d86f7"
   },
   "outputs": [],
   "source": [
    "# Plot the evolution of accuracy during training\n",
    "def plot_acc(acc_list):\n",
    "    fig = plt.figure(figsize=(6, 6))\n",
    "    ax = fig.add_subplot(111)\n",
    "    step = np.arange(1, len(acc_list)+1)\n",
    "    plt.plot(step, acc_list)\n",
    "    plt.title('Evolution of the accuracy during the training')\n",
    "    plt.xlabel('iteration')\n",
    "    plt.ylabel('Training accuracy')\n",
    "\n",
    "plot_acc(logger[\"acc\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "c22c67642f3841f69eada115294aea31",
    "deepnote_cell_type": "markdown",
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "acbd16b5d8076747c3401df17e7f3168",
     "grade": false,
     "grade_id": "cell-3c1e75ffbae29782",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Plotting results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "7cb1982dc6e64520ae9d09ce5fb6c8f6",
    "deepnote_cell_type": "markdown",
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e5adf60d50fa30976d9dca47ab659ef2",
     "grade": false,
     "grade_id": "cell-fa0f390f50b91437",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We also implemented functions in helpers.py which can help you visualize the decision boundary for this model, and visualize which points are correctly classified, and which aren't. This step is important in the process of developing your algorithm as it allows you to make sense of the mathematical result you obtained. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "8352495c091d4c07bc10032d84e5e7f6",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "deletable": false,
    "editable": false,
    "execution_millis": 703,
    "execution_start": 1679936026039,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c37f802faf34f67914be5f27a0daf657",
     "grade": false,
     "grade_id": "cell-d353fba1cc7a47ed",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "source_hash": "15977b19"
   },
   "outputs": [],
   "source": [
    "class_names = list(label_map.values())\n",
    "ax_titles = [\"Bill length (normalized)\", \"Flipper length (normalized)\"]\n",
    "helpers.plot_boundaries(X=X_train_iris,\n",
    "                        y=y_train_iris, \n",
    "                        w=w,\n",
    "                        output_func=logistic_output, \n",
    "                        class_names=class_names, \n",
    "                        ax_titles=ax_titles,\n",
    "                        train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "fcddf62a02ce4a3a8482c373272a7951",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "deletable": false,
    "editable": false,
    "execution_millis": 478,
    "execution_start": 1679936026750,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3d9cc57a71f6ad60288da50de3092868",
     "grade": false,
     "grade_id": "cell-9bf3981ac19644b9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "source_hash": "a7c46fbb"
   },
   "outputs": [],
   "source": [
    "helpers.plot_boundaries(X=X_test_iris, \n",
    "                        y=y_test_iris, \n",
    "                        w=w, \n",
    "                        output_func=logistic_output, \n",
    "                        class_names=class_names, \n",
    "                        ax_titles=ax_titles, \n",
    "                        train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "81d1f617336841e89a752d00c8f0b579",
    "deepnote_cell_type": "markdown",
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "518ebcb5185d2b2a25dc1ed88b4f6721",
     "grade": false,
     "grade_id": "cell-ac7d37609dd641b4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "If everything was implemented correctly, you'll notice that your model fits the training data with a good accuracy and also generalizes to the test data. Generalizing to new and unknown data points is the ultimate goal of any machine learning algorithm. In our case, the visualisation highlights this property."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "e2d9352bcd8049899520cd4fceac5991",
    "deepnote_cell_type": "markdown",
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "03049e5cf209e61bccc0f19f19268826",
     "grade": false,
     "grade_id": "cell-f84f855675584e9a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Training visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "66fbce4bff9644059f867473a57fa3b6",
    "deepnote_cell_type": "markdown",
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9780f14a873f7348480b58965e9d1006",
     "grade": false,
     "grade_id": "cell-ae3fbfb5e789d8c7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We alo implemented an interactive function which you can use to visualize the decision boundaries at different iterations. \n",
    "Notice how the decision boundary gradually improves during training?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "8b94c74327ba4b39999410bbbe642b86",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "deletable": false,
    "editable": false,
    "execution_millis": 532,
    "execution_start": 1679936054400,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "04f26f16e63e1ec37ed486f6d02b0c4d",
     "grade": false,
     "grade_id": "cell-9a222d121b639205",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "source_hash": "7ccdba76"
   },
   "outputs": [],
   "source": [
    "helpers.interactive_boundaries(X_train_iris, \n",
    "                               y_train_iris, \n",
    "                               X_test_iris, \n",
    "                               y_test_iris, \n",
    "                               logger[\"w\"], \n",
    "                               logistic_output, \n",
    "                               class_names, \n",
    "                               ax_titles, \n",
    "                               total_steps=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "33d63d2429a14433a7a177afb621055b",
    "deepnote_cell_type": "markdown",
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8620e8b5b98afec9497e67e7991ef9cf",
     "grade": false,
     "grade_id": "cell-726b08592101a09f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Good job on completing the first part of this exercise! In the next parts, we'll implement a slightly modified version of logistic regression and train it on a different dataset. The functions implemented so far will prove to be very useful later on, so make sure that they are correct!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "f458022117c443a69fad3eb0fb62d165",
    "deepnote_cell_type": "markdown",
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "824872de8d62f11a26c26a8eb44eca9e",
     "grade": false,
     "grade_id": "cell-52d83aaeea5761d8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Part 2: Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "a177b643c516476aa6d5345b0398b661",
    "deepnote_cell_type": "markdown",
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "44782d96bec1319d32f82d5359088bc3",
     "grade": false,
     "grade_id": "cell-d8798eda727da570",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The second part of this exercise consists of adding regularization to improve logistic regression's performance on a higher dimensional dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "329c777a79df41f89e4cc7a9a2691e54",
    "deepnote_cell_type": "markdown",
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f28c78851a21638a195b2c36882f06ed",
     "grade": false,
     "grade_id": "cell-326f23c9275b7914",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 2.1. Connectionist Bench Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "929d293fb97e4d3b976ff190d3bff784",
    "deepnote_cell_type": "markdown",
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5e096d9a37da0c1fcadf169650f4eec2",
     "grade": false,
     "grade_id": "cell-05b2d03e39ce0936",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In this part, you'll use the Connectionist Bench Dataset, where the task is to train a model to discriminate between sonar signals bounced off a mine (metal cylinder) and those bounced off a roughly cylindrical rock. Each sample is composed of 60 features and a label: \"S\" if it is a stone and \"M\" if it is Metal (mine)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "092b8887747842eea81c91502b95c21d",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "deletable": false,
    "editable": false,
    "execution_millis": 258,
    "execution_start": 1679936028327,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "777876cb65f0fded4dc26a37537b5721",
     "grade": false,
     "grade_id": "cell-d71729c72af3d1b9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "source_hash": "41671d76"
   },
   "outputs": [],
   "source": [
    "sonar = pd.read_csv(\"data/sonar.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "0ce06394eefb4188bc8c1d8f7846370c",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "deletable": false,
    "editable": false,
    "execution_millis": 356,
    "execution_start": 1679936028328,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ad0dfc42eabd59b982a65a172925dc83",
     "grade": false,
     "grade_id": "cell-eb47cc7e1eb7e8d2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "source_hash": "570b7d92"
   },
   "outputs": [],
   "source": [
    "print(f\"Shape: {sonar.shape}\")\n",
    "\n",
    "sonar.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "870a32ab7e4646029c657e004f578851",
    "deepnote_cell_type": "markdown",
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "293ad23701eb45a41da1bc5f5e85c5b5",
     "grade": false,
     "grade_id": "cell-02f0a990ec899b02",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We use `helpers.preprocess_data()` to extract a training and test set from this data. The label \"M\" (metal) is mapped to 0 and the label \"S\" (stone) is mapped to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "b975eeb09d214230a655d4820b4d7997",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "deletable": false,
    "editable": false,
    "execution_millis": 100,
    "execution_start": 1679936028589,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "629e4ea20871815e83e518667de3acec",
     "grade": false,
     "grade_id": "cell-c5f62dc1b0f00e13",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "source_hash": "7e1ef869"
   },
   "outputs": [],
   "source": [
    "X_train_sonar, y_train_sonar, X_test_sonar, y_test_sonar, feature_names, label_map = helpers.preprocess_data(sonar, label=\"target\", train_size=0.75, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "ccdef3f8c4e34c9481301e24919285db",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "deletable": false,
    "editable": false,
    "execution_millis": 98,
    "execution_start": 1679936028591,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5d902d578dc722c2e205758c8b2159b4",
     "grade": false,
     "grade_id": "cell-2d453c5fcfbae99b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "source_hash": "a908160"
   },
   "outputs": [],
   "source": [
    "label_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "946197197d0e4926991aff9c87e8111b",
    "deepnote_cell_type": "markdown",
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "50264535247cd7bbe08a18e7d8042704",
     "grade": false,
     "grade_id": "cell-4acc3330d529f05b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "You also need to normalize the data, as was done in the previous part. You can use `helpers.normalize()` to do so.\n",
    "\n",
    "**Task: Normalize `X_train_sonar` and `X_test_sonar` using `helpers.normalize()`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "11420eadc97c432aa93df6ba106bbe47",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "deletable": false,
    "execution_millis": 96,
    "execution_start": 1679936028593,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d8694feda3bcc15dc2baa5805e9038d0",
     "grade": false,
     "grade_id": "cell-f586fe01b0c8f047",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "source_hash": "dcb412ab"
   },
   "outputs": [],
   "source": [
    "# Normalize features of the training and test set using the mean and std of the training set features\n",
    "# Use helpers.normalize()\n",
    "### START CODE HERE ###\n",
    "mean = ...\n",
    "std = ...\n",
    "\n",
    "X_train_sonar = ...\n",
    "X_test_sonar = ...\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "4bbc648e9a1a453d9906f9408d225803",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "deletable": false,
    "editable": false,
    "execution_millis": 96,
    "execution_start": 1679936028593,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7e131c179a33fc9b6dfb7263c7a11784",
     "grade": true,
     "grade_id": "cell-463cbb9c12fe4e46",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "source_hash": "af32bd11"
   },
   "outputs": [],
   "source": [
    "assert np.allclose(X_train_sonar.mean(axis=0), 0)\n",
    "assert np.allclose(X_train_sonar.std(axis=0), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "1cd98d3dfccf461f9d129ea15a51afe1",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "deletable": false,
    "editable": false,
    "execution_millis": 95,
    "execution_start": 1679936028594,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2a07d3e781fb568ee530c8ca4c686e95",
     "grade": false,
     "grade_id": "cell-c49356e532add732",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "source_hash": "fb68a6d5"
   },
   "outputs": [],
   "source": [
    "#¬†Now we add a column to simplify our intercept:\n",
    "X_train_sonar = helpers.insert_offset(X_train_sonar)\n",
    "X_test_sonar = helpers.insert_offset(X_test_sonar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "6ec8548f7a87478d99308af8595d07fe",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "deletable": false,
    "editable": false,
    "execution_millis": 94,
    "execution_start": 1679936028595,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "71199284158cf99770cf6ccd190ae7bb",
     "grade": false,
     "grade_id": "cell-41ebc96fc849b70e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "source_hash": "9df2cc8f"
   },
   "outputs": [],
   "source": [
    "# Show shapes\n",
    "print('Training set shape:')\n",
    "print(f'X: {X_train_sonar.shape}, y: {y_train_sonar.shape}')\n",
    "\n",
    "print('\\nTest set shape:')\n",
    "print(f'X: {X_test_sonar.shape}, y: {y_test_sonar.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "8f3c2f2eda2e4987b9d79b5888799f4f",
    "deepnote_cell_type": "markdown",
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "10fc5d39ccadcfc5eba2b2444d164acd",
     "grade": false,
     "grade_id": "cell-894756d945f8c7b7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now that you've processed the data, let's check how well logistic regression performs without regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "9698935f984349e78e17d0995d2ae6a3",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "deletable": false,
    "editable": false,
    "execution_millis": 108,
    "execution_start": 1679936028596,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "01954837e9b4285cbb45ebd1cfff2178",
     "grade": false,
     "grade_id": "cell-6323338a95853f18",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "source_hash": "8371db2d"
   },
   "outputs": [],
   "source": [
    "w, _ = train_logistic_regression(X_train_sonar, y_train_sonar, max_iters=1001, lr=0.5, loss_freq=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "a83ced0b23c846c89557b7f040ffa12f",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "deletable": false,
    "editable": false,
    "execution_millis": 12,
    "execution_start": 1679936028693,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "69b2619b6cb5850e4cf4f8656a6e8a4a",
     "grade": false,
     "grade_id": "cell-81dd9ca3b0a31e70",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "source_hash": "bd52789d"
   },
   "outputs": [],
   "source": [
    "# Train acc\n",
    "y_hat = logistic_output(X_train_sonar, w)\n",
    "acc = accuracy(np.argmax(y_train_sonar, axis=-1), classify(y_hat))\n",
    "print(f'Train accuracy: {100 * acc:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "1dd1a75c38064f9b829d3aa743453a2d",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "deletable": false,
    "editable": false,
    "execution_millis": 12,
    "execution_start": 1679936028694,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "219a5c2bf950335adb872e25236f7fcf",
     "grade": false,
     "grade_id": "cell-3bdb1ba0ccae21dd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "source_hash": "7f10e5a3"
   },
   "outputs": [],
   "source": [
    "# Test acc\n",
    "y_hat = logistic_output(X_test_sonar, w)\n",
    "acc = accuracy(np.argmax(y_test_sonar, axis=-1), classify(y_hat))\n",
    "print(f'Test accuracy: {100*acc:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "2ccd4cc37f104820b32163822302c798",
    "deepnote_cell_type": "markdown",
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "eb314a60dcc7f59fdb19b4e369a06333",
     "grade": false,
     "grade_id": "cell-73236922dc5e11f9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 2.2. Penalized logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "aab298c87ece4820b327a24f406e7658",
    "deepnote_cell_type": "markdown",
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2086d3ef3fe063969a39de1a0dc62baf",
     "grade": false,
     "grade_id": "cell-2396bd8a75a96a80",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Penalized categorical cross-entropy loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "299b39f34f2e402b908150ef84af3b84",
    "deepnote_cell_type": "markdown",
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8198b98cf98238d26c77497e33f28d44",
     "grade": false,
     "grade_id": "cell-6b45823dd4942f23",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Logistic regression can overfit when there are too many parameters compared to training examples, as it can find weights for which the decision boundary perfectly separates all the training examples. When such overfitting occurs, the weights are often set to large values. One way to reduce such overfitting is to prevent weights from becoming so large, which can be done using L2 regularization, which consists of changing the training objective to penalize \"large\" weights. The name L2 regularization comes from the fact that weights are penalized using the L2-norm.\n",
    "\n",
    "We will call this new training objective (new loss function) the penalized categorical-cross entropy loss (or PCCE), it is expressed as:\n",
    "\n",
    "$$\n",
    "\\text{PCCE}(\\mathbf{w}) = \\text{CCE}(\\mathbf{w}) + \\lambda \\|\\mathbf{w}\\|_{2}^{2} = \\text{CCE}(\\mathbf{X}, \\boldsymbol{y}, \\mathbf{W}) = - \\frac{1}{N}\\sum^{N}_{j=1}\\sum^{C}_i {y}_j^{(i)} \\log(\\hat{y}_j^{(i)})  + \\lambda\\|\\mathbf{w}\\|_{2}^{2}\n",
    "$$\n",
    "\n",
    "As you can see, this loss function consists of adding a penalty term to CCE, which **penalizes the weights but not the bias term**. The hyper-parameter $\\lambda$ controls overfitting. The larger its value, the more the weights are penalized for being large, which makes the model less flexible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "5572cb8de80f46cb9e77d6f7628b7ef1",
    "deepnote_cell_type": "markdown",
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ee95780457017a8e1abd7516adf5abe5",
     "grade": false,
     "grade_id": "cell-0a9d69ed8d9e088c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Task: Implement `penalized_cce_loss()`.**\n",
    "\n",
    "**Note:** `lambda` is a reserved keyword in Python (used for lambda expressions), so no variables can be named this way. In our function, we use `lambd` as a variable name instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "8d5a757c4b564ca28441e433d1e71421",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "deletable": false,
    "execution_millis": 12,
    "execution_start": 1679936028697,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4e841b553dd7acb5f13448ef2f85040f",
     "grade": false,
     "grade_id": "cell-85ebdc0cd223d98d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "source_hash": "3f76f17d"
   },
   "outputs": [],
   "source": [
    "def penalized_cce_loss(X: np.ndarray,  y: np.ndarray, w: np.ndarray, lambd: float) -> float:\n",
    "    \"\"\" Penalized categorical cross-entropy loss function\n",
    "    \n",
    "    Args:\n",
    "        X: Dataset of shape (N, D)\n",
    "        y: Labels of shape (N, C).\n",
    "        w: Weights of logistic regression model of shape (D, C)\n",
    "        lambd: regularization coefficient (named this way as lambda is a reserved keyword in python)\n",
    "    \n",
    "    Returns:\n",
    "        float: categoricall cross-entropy loss.\n",
    "    \"\"\"\n",
    "    ### START CODE HERE ###\n",
    "    loss = ...\n",
    "    ### END CODE HERE ###\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "57523f7a0c22488c8416a5f1c940ef69",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "deletable": false,
    "editable": false,
    "execution_millis": 33,
    "execution_start": 1679936028711,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0d76f45d7dd4f9b60560ecf102399240",
     "grade": true,
     "grade_id": "cell-80d627a52390d834",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "source_hash": "d99e9137"
   },
   "outputs": [],
   "source": [
    "X = X_train_sonar[:3]\n",
    "y  = y_train_sonar[:3]\n",
    "w = np.ones((X.shape[1], y.shape[1]))\n",
    "\n",
    "loss = penalized_cce_loss(X, y, w, lambd=0.1)\n",
    "assert(isinstance(loss, float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "e80692b26044438a97a89c46a52a4140",
    "deepnote_cell_type": "markdown",
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6b5b2c7a88131f2a52b2ed87396f69ec",
     "grade": false,
     "grade_id": "cell-f8e5e022336e7350",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Penalized categorical cross-entropy loss gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "9dd79a21360c48b29f8dccfdd361e38e",
    "deepnote_cell_type": "markdown",
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "36de08fa91d66b86f869e2932f03fc4b",
     "grade": false,
     "grade_id": "cell-ac4bcb4b3ef034f2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In order to use the penalized categorical cross-entropy loss, you need its gradient. This time, we won't give you the gradient formula, you'll need to figure it out yourself!\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**Task: Figure out the gradient for the penalized categorical cross-entropy loss and implement `penalized_cce_gradient()`.**\n",
    "\n",
    "**Hint:** You can use the gradient of the categorical cross-entropy loss as a starting point.\n",
    "\n",
    "Here are two useful resources for matrix calculus:\n",
    "- http://www.matrixcalculus.org/\n",
    "- https://en.wikipedia.org/wiki/Matrix_calculus\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<blockquote>\n",
    "WRITE YOUR ANSWER HERE\n",
    "\n",
    "$$\\frac{\\partial \\text{PCCE}(\\mathbf{w})}{\\partial \\mathbf{w}} = $$\n",
    "\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "984cd02f019f419c9798b0ff2b3d46e4",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "deletable": false,
    "execution_millis": 22,
    "execution_start": 1679936028723,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a89e94dd2f3b0d8e07a41bb8237e433f",
     "grade": false,
     "grade_id": "cell-6f675c8a2fd8074e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "source_hash": "14c9a306"
   },
   "outputs": [],
   "source": [
    "def penalized_cce_gradient(X: np.ndarray,  \n",
    "                           y: np.ndarray, \n",
    "                           w: np.ndarray, \n",
    "                           lambd: float) -> Tuple[np.ndarray, float]:\n",
    "    \"\"\" Gradient of the penalized categorical cross-entropy loss\n",
    "    \n",
    "    Args:\n",
    "        X: Dataset of shape (N, D)\n",
    "        y: Labels of shape (N, C)\n",
    "        w: Weights of logistic regression model of shape (D, C)\n",
    "        lambd: regularization coefficient (named this way as lambda is a reserved keyword in python)\n",
    "        \n",
    "    Returns:\n",
    "        dw (np.ndarray) gradient of the loss with respect to w of shape (D, C)\n",
    "    \"\"\"\n",
    "    ### START CODE HERE ###\n",
    "    dw = ...\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return dw \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "22e8f651cd3642e6943eac9a7a15af85",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "deletable": false,
    "editable": false,
    "execution_millis": 11,
    "execution_start": 1679936028734,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "532d849d3c963a4e8846a0d7a4bf08db",
     "grade": true,
     "grade_id": "cell-9849547dd13fcd6a",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "source_hash": "4299a147"
   },
   "outputs": [],
   "source": [
    "X = X_train_sonar[:3]\n",
    "y  = y_train_sonar[:3]\n",
    "w = np.ones((X.shape[1], y.shape[1]))\n",
    "\n",
    "dw = penalized_cce_gradient(X, y, w, lambd=0.1)\n",
    "assert(dw.shape == w.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "0ee2d814c8a840bca97abbfe9f141866",
    "deepnote_cell_type": "markdown",
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "728a73a4f3d952c1da5398e29c0c2630",
     "grade": false,
     "grade_id": "cell-988814e2d1d31b3d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "f04c5ed32e8e4602bb85532b79067a71",
    "deepnote_cell_type": "markdown",
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "da3ffa264a9303b59aeb0965f27c8146",
     "grade": false,
     "grade_id": "cell-338d8519584845d2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now that you've implemented the loss function and its gradient, you can use it to train your model.\n",
    "\n",
    "**Task: Implement `train_penalized_logistic_regression()`.**\n",
    "\n",
    "**Hint:** This training function is very similar to the one implemented in Part 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "872d26b306bb43718ef8f2437543b01b",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "deletable": false,
    "execution_millis": 2,
    "execution_start": 1679936028791,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "df530829efe4ddda2ca0d93e0c3e77b3",
     "grade": false,
     "grade_id": "cell-030119f67fac9804",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "source_hash": "d09fbd2a"
   },
   "outputs": [],
   "source": [
    "def train_penalized_logistic_regression(X: np.ndarray, \n",
    "                                        y: np.ndarray, \n",
    "                                        lambd: float, \n",
    "                                        max_iters: int = 1001, \n",
    "                                        lr: float = 0.5, \n",
    "                                        loss_freq: int = 0) -> Tuple[np.ndarray, float, dict]:\n",
    "    \"\"\" Training function for multi-class penalized logistic regression using gradient descent\n",
    "    \n",
    "    Args:\n",
    "        X: Dataset of shape (N, D).\n",
    "        y: Labels of shape (N, C).\n",
    "        lambd: regularization coefficient (named this way as lambda is a reserved keyword in python)\n",
    "        max_iters: Maximum number of iterations.\n",
    "        lr: The learning rate of  the gradient step.\n",
    "        loss_freq : Prints the loss every `loss_freq` iterations.\n",
    "        \n",
    "    Returns:\n",
    "        w: weights of shape (D, C)\n",
    "        viz_d: dict used for visualizations\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize weights\n",
    "    np.random.seed(0)\n",
    "    y_class = convert_onehot_to_class(y)\n",
    "    w = np.random.normal(0, 1, size=(X.shape[1], y.shape[1]))\n",
    "    \n",
    "    # Initialize dict with lists to keep track of loss, accuracy, weight and bias evolution\n",
    "    logger = {'loss': [], \n",
    "             'acc': [], \n",
    "            }\n",
    "    \n",
    "    \n",
    "    for i in range(max_iters):\n",
    "        # Compute loss and dw and update w\n",
    "        ### START CODE HERE ###\n",
    "        loss = ...\n",
    "        dw = ...\n",
    "        \n",
    "        w = ...\n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Keep track of loss and accuracy values for each iteration\n",
    "        logger['loss'].append(loss)\n",
    "        \n",
    "        y_hat = logistic_output(X, w)\n",
    "        logger['acc'].append(accuracy(y_class, classify(y_hat)))\n",
    "        \n",
    "        if (loss_freq !=0) and i % loss_freq == 0:\n",
    "            print(f'Loss at iter {i}: {loss:.5f}')\n",
    "        \n",
    "    if (loss_freq != 0):\n",
    "        print('\\nFinal loss: {:.5f}'.format(logger['loss'][-1]))\n",
    "        \n",
    "    return w, logger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "041ce13e678c45f497d2474be8e95093",
    "deepnote_cell_type": "markdown",
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5bb9b1ddbb80802a3a324039c5cd1002",
     "grade": false,
     "grade_id": "cell-6d4df3995d747ec2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let's now the check that there are no huge mistakes in your implementation by training your model for two different values of $\\lambda$ and checking the training accuracy. Given that regularization reduces overfitting, you should expect the training accuracy to decrease when $\\lambda$ increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "5f814062d4c14b9a9ec2b52661687c92",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "deletable": false,
    "editable": false,
    "execution_millis": 203,
    "execution_start": 1679936028792,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dddc4ee3397bc5a677b1cc6d8b5b07b5",
     "grade": true,
     "grade_id": "cell-901ce6821fb221df",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "source_hash": "2a592862"
   },
   "outputs": [],
   "source": [
    "w, _ = train_penalized_logistic_regression(X_train_sonar, y_train_sonar, lambd=0, max_iters=1001, lr=0.5, loss_freq=100)\n",
    "y_hat = logistic_output(X_train_sonar, w)\n",
    "acc = accuracy(convert_onehot_to_class(y_train_sonar), classify(y_hat))\n",
    "print(f'Train accuracy: {100*acc:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "c824381988544fa78f3e5754420201e5",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 228,
    "execution_start": 1679936028993,
    "source_hash": "4fd955b"
   },
   "outputs": [],
   "source": [
    "w, _ = train_penalized_logistic_regression(X_train_sonar, y_train_sonar, lambd=0.1, max_iters=1001, lr=0.5, loss_freq=100)\n",
    "y_hat = logistic_output(X_train_sonar, w)\n",
    "acc = accuracy(convert_onehot_to_class(y_train_sonar), classify(y_hat))\n",
    "print(f'Train accuracy: {100*acc:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "85cf4ada69f44b1197675cafc5eda004",
    "deepnote_cell_type": "markdown",
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "65204b2051649647e09ebda185901c82",
     "grade": false,
     "grade_id": "cell-5a09569774f78f7b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now that you've implemented penalized logistic regression, you may wonder which value of $\\lambda$ to pick. This is what we'll try to figure out in part 3, using cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "fecf9521d3b24c81a197e3c32cc02cfe",
    "deepnote_cell_type": "markdown",
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "662575b52f8087423cb56b980f6cdd5d",
     "grade": false,
     "grade_id": "cell-3a7c5434a60cdae6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Part 3: Cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "746f3014a82946d3924d73e6f10b7c9d",
    "deepnote_cell_type": "markdown",
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b65e945967c5267becfd9a8e891fb438",
     "grade": false,
     "grade_id": "cell-e986c697c7149a78",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In this class, you saw that a dataset is usually split into 3 parts: One training set, one validation set and one test set. The training set is used as training data, the validation set is used for tuning hyper-parameters and the test set is held out for final evaluation. However, by partitioning data this way, we reduce the number of samples available for training the model, and the results on the validation depend on a particular random choice for the training and validation sets. For datasets with a small amount of training examples, this can be especially problematic.\n",
    "\n",
    "\n",
    "This is where cross-validation comes into play. With cross-validation, a test set will still be held out for final evaluation, but there is no need for a designated validation set. Here, you will implement a non-exhaustive cross-validation technique known as k-fold cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "0003787710fa4e04a3678513632c0966",
    "deepnote_cell_type": "markdown",
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7bb7901699127361cb9eb0bc6d38ceae",
     "grade": false,
     "grade_id": "cell-7d4c7380ddc963c2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 3.1. k-Fold Cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "8064cfcbbd154864910d43b2d0301356",
    "deepnote_cell_type": "markdown",
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "21a9ac2014d3a1ec195b96d187ae14a6",
     "grade": false,
     "grade_id": "cell-d82878f7c4c982a1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In k-fold cross-validation, the training data is randomly partitioned into $k$ equal sized subsamples. Of the $k$ subsamples, a single subsample is used as the validation data, and the remaining $k-1$ subsamples are used as training data. This process is repeated $k$ times, with each of the $k$ subsamples used exactly once as the validation data. The $k$ results are then averaged to produce a single estimation. \n",
    "\n",
    "This process is illustrated below:\n",
    "\n",
    "<img src=\"images/kfold_cv.png\" alt=\"kfold\" style=\"width:500px\"/>\n",
    "\n",
    "We have implemented the function `k_fold_indices()` for you, which generates indices for k-fold cross-validation. You can see its implementation and an example usage in the cell below.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**Discuss:** Explain the purpose of the validation set in the context of k-fold cross-validation. How is the validation set utilized during each iteration of the cross-validation process?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> WRITE YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "8955ce17dd594845a3b8f2e1a57389f8",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "deletable": false,
    "editable": false,
    "execution_millis": 22,
    "execution_start": 1679936029226,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "23f91c6024704ede9159818790935860",
     "grade": false,
     "grade_id": "cell-1725cb36bdc5a551",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "source_hash": "91e55176"
   },
   "outputs": [],
   "source": [
    "def k_fold_indices(num_examples: int, k: int = 4) -> List[Tuple[np.ndarray, np.ndarray]]:\n",
    "    \"\"\"Generates indices for k-fold cross-validation\n",
    "\n",
    "    Args:\n",
    "        num_examples: Number of training examples\n",
    "        k: Number of folds\n",
    "\n",
    "    Returns:\n",
    "        List of tuples containing the training indices and validation indices\n",
    "\n",
    "    \"\"\"\n",
    "    indices = np.arange(num_examples)\n",
    "    split_size = num_examples // k\n",
    "    val_indices = [indices[k * split_size : (k + 1) * split_size] for k in range(k)]\n",
    "    both_indices = [(np.delete(indices, val_ind), val_ind) for val_ind in val_indices]\n",
    "    return both_indices\n",
    "\n",
    "# Example usage\n",
    "for train_index, val_index in k_fold_indices(num_examples=8, k=4):\n",
    "    # Do something with the indices\n",
    "    print(f\"{train_index} {val_index}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "bd20278d0b3b4381a2e1e3e6921b406c",
    "deepnote_cell_type": "markdown",
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "66ee08aa5f173c1073458147244212ed",
     "grade": false,
     "grade_id": "cell-ba4885296a47fae1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Task: With the help of `k_fold_indices()` and  previously implemented functions, implement the function `cross_val_penalized_logistic_regression()` according to its documentation.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "7e2f204199b14b50ba56b9f73a9cd8ed",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "deletable": false,
    "execution_millis": 18,
    "execution_start": 1679936029231,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4c2f5396a5ad0fc93d2a352524ef1e91",
     "grade": false,
     "grade_id": "cell-0f739fc8018fc2d6",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "source_hash": "e72752ac"
   },
   "outputs": [],
   "source": [
    "def cross_val_penalized_logistic_regression(X: np.ndarray,\n",
    "                                            y: np.ndarray,\n",
    "                                            lambd: float = 0,\n",
    "                                            max_iters: int = 1001,\n",
    "                                            lr: float = 0.5,\n",
    "                                            loss_freq: int = 0,\n",
    "                                            k: int = 4) -> float:\n",
    "    \"\"\"\n",
    "    Performs k-fold cross-validation for penalized logistic regression and returns the mean validation accuracy\n",
    "\n",
    "    Args:\n",
    "        X: Dataset of shape (N, D).\n",
    "        y: Labels of shape (N, C).\n",
    "        lambd: regularization coefficient (named this way as lambda is a reserved keyword in python)\n",
    "        max_iters: Maximum number of iterations.\n",
    "        lr: The learning rate of  the gradient step.\n",
    "        loss_freq : Prints the loss every `loss_freq` iterations.\n",
    "        k: Number of folds\n",
    "\n",
    "    Returns:\n",
    "        Mean validation accuracy\n",
    "\n",
    "    \"\"\"\n",
    "    val_accs = []\n",
    "    \n",
    "    # Hint: Use a for-loop to iterate over all k-fold indices. For each fold, use train split to train, and val split to get accuracy\n",
    "    ### START CODE HERE ###\n",
    "    ...\n",
    "        \n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return np.mean(val_accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "5d79ed36ca2f4280852de28492c32a55",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "deletable": false,
    "editable": false,
    "execution_millis": 653,
    "execution_start": 1679936029298,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c379d5afe144613bb96968939accc62b",
     "grade": true,
     "grade_id": "cell-7374c5c917d19ae3",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "source_hash": "72e24a9c"
   },
   "outputs": [],
   "source": [
    "mean_cv_acc = cross_val_penalized_logistic_regression(X_train_sonar, y_train_sonar)\n",
    "print(f\"Mean CV acc for default settings: {mean_cv_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "91899cda32944b2eaf099723c9948126",
    "deepnote_cell_type": "markdown",
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a6390bbc2a9de93b9c6f64cf75ddbbe6",
     "grade": false,
     "grade_id": "cell-7cb106f8ed2aba6f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 3.2. Finding a good regularization parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "fa84a251ae814988ab97a8a440ec4a4d",
    "deepnote_cell_type": "markdown",
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5ce1ea5cf8c6a85598c8ae56214afd66",
     "grade": false,
     "grade_id": "cell-6397799822bc5810",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "You'll now use cross-validation to find a good $\\lambda$ for penalized logistic regression.\n",
    "\n",
    "**Task: Find the best value of $\\lambda$**\n",
    "\n",
    "To do so:\n",
    "- Suppose that `max_iters` and `lr` are set to the **default values** of `cross_val_penalized_logistic_regression()`, and that you can only modify the hyper-parameter `lambd`.\n",
    "- Compute the 4-fold cross-validation accuracy for $\\lambda \\in \\{0, 0.001, 0.01, 0.1, 1\\}.$\n",
    "- Set `best_cv_acc` to the best cross-validation accuracy obtained, and `best_lambda` to the $\\lambda$ associated with the best cross-validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "5a0117cf51c14341a2ace01f8dcf5cb0",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "deletable": false,
    "execution_millis": 3286,
    "execution_start": 1679936030071,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "849879cdf9351bf052050b32edd74040",
     "grade": false,
     "grade_id": "cell-50cb47cb492159f6",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "source_hash": "7d120b03"
   },
   "outputs": [],
   "source": [
    "# Only search for values in this list, do not modify  it\n",
    "lambdas = [0, 0.001, 0.01, 0.1, 1]\n",
    "k = 4\n",
    "\n",
    "# cv_accs should contain the mean cross-validation accuracy for each value of lambda\n",
    "cv_accs = []\n",
    "### START CODE HERE ##\n",
    "...\n",
    "### END CODE HERE ###\n",
    "\n",
    "for lambd, acc in zip(lambdas, cv_accs):\n",
    "    print(f\"Lambda: {lambd}\")\n",
    "    print(f\"Cross-val acc: {acc}\")\n",
    "    print()  \n",
    "\n",
    "### START CODE HERE ###\n",
    "best_cv_acc = ...\n",
    "best_lambda = ...\n",
    "### END CODE HERE ###\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "b5ddfa7d54f04641b2a2a2d35955031f",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "deletable": false,
    "editable": false,
    "execution_millis": 5,
    "execution_start": 1679936033379,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0d1658c5ff395125ff67de211f4ca50c",
     "grade": true,
     "grade_id": "cell-252967d41a7626da",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "source_hash": "126bfd20"
   },
   "outputs": [],
   "source": [
    "print(f\"Best lambda: {best_lambda}\")\n",
    "print(f\"Best CV acc: {best_cv_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "444b73b439d9470fa34f89cdb8285193",
    "deepnote_cell_type": "markdown",
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "78e68d41caefca3c01838a7e8754407b",
     "grade": false,
     "grade_id": "cell-40fa5192703e964f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now that you've settled on a value for $\\lambda$, you can use it to train our model using our entire training set, and find out how well it performs on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "f1626e13076440f29dfd77d317aef754",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 172,
    "execution_start": 1679936033383,
    "source_hash": "f549eaeb"
   },
   "outputs": [],
   "source": [
    "w, _ = train_penalized_logistic_regression(X_train_sonar, y_train_sonar, lambd=best_lambda, max_iters=1001, lr=0.5, loss_freq=100)\n",
    "\n",
    "# Test acc\n",
    "y_hat = logistic_output(X_test_sonar, w)\n",
    "acc = accuracy(convert_onehot_to_class(y_test_sonar), classify(y_hat))\n",
    "print(f'Test accuracy: {100*acc:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "caa2a3c4ba85496b836a1b6d0607b4b7",
    "deepnote_cell_type": "markdown",
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f94f7089bf30660babc979e7602c7591",
     "grade": false,
     "grade_id": "cell-4323a262e8531393",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Assuming that penalized logistic regression, cross-validation and the hyperparameter search were implemented correctly, your test accuracy should be slightly higher than what was obtained for standard logistic regression in Part 2.1. While this is a modest increase, regularization can lead to much more significant increases in accuracy when working with complex machine learning models such as neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "18616b76916d4ae28141db60ddb26e37",
    "deepnote_cell_type": "markdown",
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "38a71709288751e63d091cba6f377d64",
     "grade": false,
     "grade_id": "cell-e8387296dc927258",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Part 4: Evaluation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "3028a7f7ea684e98b4fec77fe445ab69",
    "deepnote_cell_type": "markdown",
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "53716791d65c3d0799e499165f702d87",
     "grade": false,
     "grade_id": "cell-dcf57e8c6d8dbf36",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Accuracy is not the only metric which can be used to measure how well your model is doing. In fact, accuracy can sometimes be a very poor metric to judge model performance. For example, suppose that you are building a machine learning model to detect whether or not someone has a disease. This disease is quite rare, so you estimate that only 1% of the people tested actually have the disease. In such a scenario, a model that always classifies a patient as negative would have an accuracy of 99%, and yet that model would be completely useless (and potentially very harmful), as it would fail to detect any positive case.\n",
    "\n",
    "You'll now implement other metrics, which are usually preferable over accuracy when working on class-imbalanced datasets.\n",
    "\n",
    "\n",
    "## 4.0 Confusion Matrix\n",
    "\n",
    "A confusion matrix is a great way to visualize where you are losing accuracy and making mistakes. Indeed, some classes may be more difficult to predict correctly, or be easily mistaken for another one by your model. The confusion matrix's diagonal corresponds to correct predictions of your classes. The off-diagonal express in which class you have falsely predicted your label. Here is an example for 3 classes: \n",
    "\n",
    "|                    | Actual Class (0)    | Actual Class (1)    | Actual Class (2)    |\n",
    "|--------------------|---------------------|---------------------|---------------------|\n",
    "| **Predicted Class (0)** | True predict (0)  | False predict (1) |  False predict (2) |\n",
    "| **Predicted Class (1)** | False predict (0) | True predict (1)  | False predict (2) |\n",
    "| **Predicted Class (2)** | False predict (0) | False predict (1)  | True predict (2) |\n",
    "\n",
    "-\n",
    "-\n",
    "-\n",
    "-\n",
    "-\n",
    "\n",
    "\n",
    "**Task: Implement `confusion_matrix()`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "61208cf9baa74564bc97f47dc507fb47",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "deletable": false,
    "execution_millis": 12,
    "execution_start": 1679936033563,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a08ab603858a60688029483dfea76360",
     "grade": false,
     "grade_id": "cell-42630324c49b87f7",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "source_hash": "3a756822",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def confusion_matrix(labels_class_gt: np.ndarray, labels_pred: np.ndarray) -> np.ndarray:\n",
    "    \"\"\" Compute the confusion matrix where lines are predicted classes and columns are ground truths.\n",
    "\n",
    "    Args:\n",
    "        labels_class_gt: class labels (ground-truth) of shape (M, ).\n",
    "        labels_pred: Predicted class labels of shape (M, ).\n",
    "\n",
    "    Return:\n",
    "        np.ndarray: Confusion matrix of shape (C, C).\n",
    "    \"\"\"\n",
    "    C = len(np.unique(labels_class_gt)) # Number of unique classes found in ground_truths\n",
    "    confusion = np.zeros((C, C))\n",
    "    ### START CODE HERE ###\n",
    "    ...\n",
    "\n",
    "    return confusion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "66577e46815f4bbfa1d0756f3310ff3f",
    "deepnote_cell_type": "markdown",
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "62ea7a2d68f882c6a54611741804802a",
     "grade": false,
     "grade_id": "cell-184b75a8c882173f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 4.1. Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "ad1736d9111f40a08ec48074a5c58c87",
    "deepnote_cell_type": "markdown",
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5f1d134ebb6dc15096af6ebca142a514",
     "grade": false,
     "grade_id": "cell-3b572e534d31110b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Precision attempts to answer the following question: *What proportion of positive identifications of a class was actually correct?*\n",
    "For a single class (i), it is defined as: $$\\text{Precision} = \\frac{TP(i)}{TP(i) + \\sum_{j\\neq i}FP_i(j)}$$ , where $TP(i)$ is the true predictions of $i$ and $FP_i(j)$ is the predictions of i when actual class is meant to be j.\n",
    "\n",
    "We will now implement metrics by taking the confusion matrix as input. While this is not the customary way, it will help you fully understand the concept of confusion matrix.\n",
    "\n",
    "\n",
    "\n",
    "**Task: Implement `precision()`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "59b931e289194529b2d3bda8ea8e097a",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "deletable": false,
    "execution_millis": 4,
    "execution_start": 1679936033602,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7b51394a6e3b080a9312bb4890c1881a",
     "grade": false,
     "grade_id": "cell-92217ef8806ba33a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "source_hash": "9e1ce94a"
   },
   "outputs": [],
   "source": [
    "def precision(confusion: np.ndarray, class_ind: int) -> float:\n",
    "    \"\"\"Computes precision for class i using the confusion matrix.\n",
    "\n",
    "    Args:\n",
    "        confusion: confusion matrix of shape (C, C).\n",
    "        class_ind: class index for which we measure the metric.\n",
    "\n",
    "    Returns:\n",
    "        float: Precision, in range [0, 1].\n",
    "    \"\"\"\n",
    "    # In your solution, you may need \"if denominator == 0: return 0\"  to avoid mathematical errors\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    return ...\n",
    "\n",
    "    ### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "e68b514c8c2b46c7af335339450b939b",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "deletable": false,
    "editable": false,
    "execution_millis": 3,
    "execution_start": 1679936033603,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "84a76ca3a6330dd78b929dbcb5f91795",
     "grade": true,
     "grade_id": "cell-8773a46e5a40a46d",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "source_hash": "1c89e13a"
   },
   "outputs": [],
   "source": [
    "# Check that output is in [0, 1] (ensures that output is not a percentage)\n",
    "assert 0.0 <= precision(np.array([[1,0],[0,1]]), class_ind=0) <= 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "d836b903c42843bbb45384a727df18a2",
    "deepnote_cell_type": "markdown",
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cebdbd6818f869622d9ded19f855575f",
     "grade": false,
     "grade_id": "cell-0fc494479e6ed7b1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 4.2. Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "dc673dc225e64aa7b5b07bdef63dc2a2",
    "deepnote_cell_type": "markdown",
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "96f598d55d7c45ea46eac9f68c5a64b6",
     "grade": false,
     "grade_id": "cell-8219c0b31817a097",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Recall attempts to answer the following question: *What proportion of actual positives was identified correctly?*\n",
    "\n",
    "For a class i,\n",
    "it is defined as: $$\\text{Recall}_i = \\frac{TP(i)}{TP(i) + \\sum_jFP_j(i)}$$\n",
    "\n",
    "**Task: Implement `recall()`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "b0aa3a13a40d4447975b785f7fbfb6e4",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "deletable": false,
    "execution_millis": 2,
    "execution_start": 1679936033604,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7db53ba2bd41d35b4aef227dfef54cb6",
     "grade": false,
     "grade_id": "cell-a1f5177e65f3238a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "source_hash": "fea82fe4"
   },
   "outputs": [],
   "source": [
    "def recall(confusion: np.ndarray, class_ind: int) -> float:\n",
    "    \"\"\"Computes recall for class i using the confusion matrix.\n",
    "\n",
    "    Args:\n",
    "        confusion: confusion matrix of shape (C, C).\n",
    "        class_ind: class index for which we measure the metric.\n",
    "\n",
    "    Returns:\n",
    "        float: Recall, in range [0, 1].\n",
    "    \"\"\"\n",
    "    # In your solution, you may need \"if denominator == 0: return 0\"  to avoid mathematical errors\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    return ...\n",
    "    ### END CODE HERE ###\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "45aaffbab83a460496c2714521baadd2",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "deletable": false,
    "editable": false,
    "execution_millis": 2,
    "execution_start": 1679936033608,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f0abea049b636f00c0eb8ae77cf85bed",
     "grade": true,
     "grade_id": "cell-32e0c7c587dcf0c4",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "source_hash": "a07e402d"
   },
   "outputs": [],
   "source": [
    "# Check that output is in [0, 1] (ensures that output is not a percentage)\n",
    "assert 0.0 <= recall(np.array([np.array([1, 0]),np.array([1, 1])]),1) <= 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "91f97fb711504c3c81939d385851224f",
    "deepnote_cell_type": "markdown",
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7d8ed4b0eca6bde93930df89bceb477c",
     "grade": false,
     "grade_id": "cell-70053ddf8ce1df8e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 4.3. F1-score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "6e3e9657a03346d7ba3fe6a07c2c87fe",
    "deepnote_cell_type": "markdown",
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0ba8c4855af248c93e8766de089dfe2c",
     "grade": false,
     "grade_id": "cell-5e8418bcd90fca88",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The F1-score (or F-measure) is the harmonic mean of precision and recall: \n",
    "\n",
    "$$F_1 = 2 \\cdot \\frac{\\text{precision} \\cdot \\text{recall}}{\\text{precision} +  \\text{recall}}$$\n",
    "\n",
    "**Task: Implement `f1_score()`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "ba194659ced54cf89c585b63d2c7f42e",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "deletable": false,
    "execution_millis": 5,
    "execution_start": 1679936033611,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "625f5c119642432c22c75992a1b5c952",
     "grade": false,
     "grade_id": "cell-de868a6a6bccbc36",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "source_hash": "1bcd3a75"
   },
   "outputs": [],
   "source": [
    "def f1_score(confusion: np.ndarray, class_ind: int) -> float:\n",
    "    \"\"\"Computes the F1-score given a class i and a confusion matrix.\n",
    "\n",
    "    Args:\n",
    "        confusion: confusion matrix of shape (C, C).\n",
    "        class_ind: class index for which we measure the metric.\n",
    "\n",
    "    Returns:\n",
    "        float: F1 Score, in range [0, 1].\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    return ...\n",
    "    ### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "b80cf17e9b664963afe95629175742f0",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "deletable": false,
    "editable": false,
    "execution_millis": 4,
    "execution_start": 1679936033615,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "46764c08ddf2b8759ded7bb70dd8d5ee",
     "grade": true,
     "grade_id": "cell-794b2af85a384bc9",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "source_hash": "295d73cb"
   },
   "outputs": [],
   "source": [
    "# Check that output is in [0, 1] (ensures that output is not a percentage)\n",
    "assert 0.0 <= f1_score(np.array([[1, 0], [1, 1]]),1) <= 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "b8b619fc7e9545b2b5e62746382d4e72",
    "deepnote_cell_type": "markdown",
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4e7591e23bd6c3a8764b02d4245815dd",
     "grade": false,
     "grade_id": "cell-e8adc2d1d44f65ee",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 4.4 Balanced Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "68967d131116416884f1a401fe25a41d",
    "deepnote_cell_type": "markdown",
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "abde4016a6604d65db0d57805d53821d",
     "grade": false,
     "grade_id": "cell-163c8f2e754012e6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The balanced accuracy is calculated as the average of the proportion of correctly classified examples for each class, it can be a good alternative to accuracy for heavily imbalanced datasets. Remember the \"proportion\" of correct classification is the recall. Therefore, balanced accuracy is the averaged recall of all classes. \n",
    "\n",
    "**Task: Implement `balanced_accuracy()`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "9f2ac62434a3408fa9c0a77dd453db3b",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "deletable": false,
    "execution_millis": 2,
    "execution_start": 1679936033623,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d3b452bc44965acd50c0fff8b77b91ab",
     "grade": false,
     "grade_id": "cell-1982581c0b739223",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "source_hash": "68bc86c0"
   },
   "outputs": [],
   "source": [
    "def balanced_accuracy(confusion: np.ndarray) -> float:\n",
    "    \"\"\"Computes the balanced accuracy\n",
    "\n",
    "    Args:\n",
    "        confusion: confusion matrix of shape (C, C).\n",
    "\n",
    "    Returns:\n",
    "        float: Balanced accuracy, in range [0, 1].\n",
    "    \"\"\"\n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    return ...\n",
    "    ### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "f5901372b11c43a9ae46a21210d1c369",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "deletable": false,
    "editable": false,
    "execution_millis": 2,
    "execution_start": 1679936193716,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e496a70c77091fe7406a79bbba08b858",
     "grade": true,
     "grade_id": "cell-91e4b7493fe4db20",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "source_hash": "d28ca8e5"
   },
   "outputs": [],
   "source": [
    "# Check that output is in [0, 1] (ensures that output is not a percentage)\n",
    "assert 0.0 <= balanced_accuracy(confusion_matrix(np.array([1, 0]),np.array([1, 1]))) <= 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "ce953fc5f938432fa6f76459487c9938",
    "deepnote_cell_type": "markdown",
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8eb45c49c6746380f16f97e28f925d18",
     "grade": false,
     "grade_id": "cell-44edad22feaf448f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 4.5. Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "ccac3d4ebcbd44f48477486372c3dca7",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "deletable": false,
    "editable": false,
    "execution_millis": 3,
    "execution_start": 1679936033732,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8a415d4d61e4504be25eabcdf0b469a2",
     "grade": false,
     "grade_id": "cell-84a8d2d9732457e3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "source_hash": "2220ec2f"
   },
   "outputs": [],
   "source": [
    "print(\"Performance of penalized logistic regression:\")\n",
    "y_test_class = convert_onehot_to_class(y_test_sonar)\n",
    "confusion = confusion_matrix(y_test_class, classify(y_hat))\n",
    "# Acc\n",
    "acc = accuracy(y_test_class, classify(y_hat))\n",
    "print(f'Test accuracy: {100 * acc:.2f}%')\n",
    "# Balanced accuracy\n",
    "balanced_acc = balanced_accuracy(confusion)\n",
    "print(f'Test balanced accuracy: {100 * balanced_acc:.2f}%')\n",
    "# Per class details:\n",
    "for i in range(len(np.unique(y_test_class))):\n",
    "    print(f'Class {i} details:')\n",
    "    # Precision per class\n",
    "    prec = precision(confusion, i)\n",
    "    print(f'   - Test precision: {100 * prec:.2f}%')\n",
    "    # Recall per class\n",
    "    rec = recall(confusion, i)\n",
    "    print(f'   - Test recall: {100 * rec:.2f}%')\n",
    "    # F1-Score per class\n",
    "    f1 = f1_score(confusion, i)\n",
    "    print(f'   - Test F1-score: {100 * f1:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "cdf01b8fab1441199fb36ca16c02ec74",
    "deepnote_cell_type": "markdown",
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0969fd857852dd5d1c52a3a82fc46da5",
     "grade": false,
     "grade_id": "cell-f002639df1f0ab28",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "When using penalized logistic regression on the sonar dataset, all these metrics should give relatively similar results. The main reason for this is that this dataset is relatively well-balanced, and a lot of the implemented metrics are mostly very useful on class-imbalanced datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "8767a09601b24ccba7501905794a5848",
    "deepnote_cell_type": "markdown",
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7037a8617d8b557721d9a72b8b24c9fa",
     "grade": false,
     "grade_id": "cell-bf9ce629ae40cfd9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Whew, you reached the end of this homework! The only remaining step is to submit it, so make sure to read the next section carefully."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "fc93a3c18b6948c99775cc21c4aef8c6",
    "deepnote_cell_type": "markdown",
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2fb559447c0254c11e787c4153a05d78",
     "grade": false,
     "grade_id": "cell-5ecf4fbe4a25b341",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Submitting your homework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "04743b84a3234a02981dbb29f79fcb14",
    "deepnote_cell_type": "markdown",
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "49bcca144b5441119af224d342c87015",
     "grade": false,
     "grade_id": "cell-f78c02ebbbc1e034",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Restarting the kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "77118a6217aa416da93f45f5aff9198a",
    "deepnote_cell_type": "markdown",
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "002df2dced08d4d6980890744e828de5",
     "grade": false,
     "grade_id": "cell-28682783150761b1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Before submitting, make sure that there are no [hidden states](https://github.com/vita-epfl/introML-2021/blob/main/exercises/00-setup/jupyter.md#the-kernel) by restarting your kernel and running all cells. Check that the code runs without errors.\n",
    "\n",
    "<div class=\"alert alert-danger\">\n",
    "\n",
    "**Warning:** Make sure all results have been print. We will not grade your exercise otherwise.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "f85864d801fc405aa1b5481dcb42e83d",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "deletable": false,
    "editable": false,
    "execution_millis": 2,
    "execution_start": 1679936033733,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e9c69666b5f5efe8a98f35fa44937e51",
     "grade": false,
     "grade_id": "cell-1a6af02d5247d6e8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "source_hash": "8b8551e7"
   },
   "outputs": [],
   "source": [
    "# Restart your kernel and run all cells, make sure that you can reach this cell\n",
    "print(\"Ran all cells :)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "554405393c994c0a98a12d4d4a46acc6",
    "deepnote_cell_type": "markdown",
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9b8dd99dbf07782e20907739dadb30e2",
     "grade": false,
     "grade_id": "cell-1bc8ed4282108cd2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Renaming this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "11eea5d4a5bc44e9bb56719d167d1454",
    "deepnote_cell_type": "markdown",
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "333128acbee0eefe9a848cc5b3822d1e",
     "grade": false,
     "grade_id": "cell-15d9aa68e129fe7d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Rename this notebook by preprending your SCIPER (name it **SCIPER_logistic_regression.ipynb**). To do so, right click on the notebook in the left sidebar and select **\"Rename\"**.\n",
    "\n",
    "For example, if your SCIPER is 123456, then name your notebook *123456_logistic_regression.ipynb* ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "15ddd9694c264a7b9b94379373c7e494",
    "deepnote_cell_type": "markdown",
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "679323ee8edb108338cc57ec7d0ae93a",
     "grade": false,
     "grade_id": "cell-3e14163d48b58d49",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Submitting on Moodle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "0acc646217ad4186baf619f09a8f5a79",
    "deepnote_cell_type": "markdown",
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "00dbec5763f0b91a8ee009db50502e72",
     "grade": false,
     "grade_id": "cell-14941fad0cba56b2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Finally, make sure to save your notebook (File -> Save Notebook), so that it has your most recent changes. Then:\n",
    "- If you are working on a **local** environment, you can directly upload this notebook (i.e. the file you are currently working on) to Moodle.\n",
    "- If you are using a **cloud-based** environment such as **EPFL Noto**, you'll first need to get a local copy by right clicking on the notebook in the sidebar and clicking on **\"Download\"**, then upload the downloaded notebook to Moodle."
   ]
  }
 ],
 "metadata": {
  "deepnote": {},
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "ae3012db5db64d7d94b261673f5581df",
  "deepnote_persisted_session": {
   "createdAt": "2023-03-27T17:32:13.040Z"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
